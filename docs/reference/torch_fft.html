<!-- Generated by pkgdown: do not edit by hand -->
<!DOCTYPE html>
<html lang="en">
  <head>
  <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<title>Fft — torch_fft • torchr</title>


<!-- jquery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js" integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin="anonymous"></script>
<!-- Bootstrap -->
<link href="https://cdnjs.cloudflare.com/ajax/libs/bootswatch/3.3.7/united/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous" />


<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha256-U5ZEeKfGNOja007MMD3YBI0A3OSZOQbeG6z2f2Y0hu8=" crossorigin="anonymous"></script>

<!-- Font Awesome icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.7.1/css/all.min.css" integrity="sha256-nAmazAk6vS34Xqo0BSrTb+abbtFlgsFK7NKSi6o7Y78=" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.7.1/css/v4-shims.min.css" integrity="sha256-6qHlizsOWFskGlwVOKuns+D1nB6ssZrHQrNj1wGplHc=" crossorigin="anonymous" />

<!-- clipboard.js -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.4/clipboard.min.js" integrity="sha256-FiZwavyI2V6+EXO1U+xzLG3IKldpiTFf3153ea9zikQ=" crossorigin="anonymous"></script>

<!-- headroom.js -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.9.4/headroom.min.js" integrity="sha256-DJFC1kqIhelURkuza0AvYal5RxMtpzLjFhsnVIeuk+U=" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.9.4/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script>

<!-- pkgdown -->
<link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script>




<meta property="og:title" content="Fft — torch_fft" />
<meta property="og:description" content="Fft" />
<meta name="twitter:card" content="summary" />




<!-- mathjax -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script>

<!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->



  </head>

  <body>
    <div class="container template-reference-topic">
      <header>
      <div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">torchr</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="Released version">0.0.1.9000</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="../index.html">
    <span class="fas fa fas fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="../articles/extending-autograd.html">Extending Autograd</a>
    </li>
    <li>
      <a href="../articles/indexing.html">Indexing tensors</a>
    </li>
    <li>
      <a href="../articles/using-autograd.html">Using autograd</a>
    </li>
  </ul>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://github.com/mlverse/torch">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
      
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

      

      </header>

<div class="row">
  <div class="col-md-9 contents">
    <div class="page-header">
    <h1>Fft</h1>
    <small class="dont-index">Source: <a href='https://github.com/mlverse/torch/blob/master/R/gen-namespace-docs.R'><code>R/gen-namespace-docs.R</code></a>, <a href='https://github.com/mlverse/torch/blob/master/R/gen-namespace-examples.R'><code>R/gen-namespace-examples.R</code></a></small>
    <div class="hidden name"><code>torch_fft.Rd</code></div>
    </div>

    <div class="ref-description">
    <p>Fft</p>
    </div>


    <h2 class="hasAnchor" id="arguments"><a class="anchor" href="#arguments"></a>Arguments</h2>
    <table class="ref-arguments">
    <colgroup><col class="name" /><col class="desc" /></colgroup>
    <tr>
      <th>input</th>
      <td><p>(Tensor) the input tensor of at least <code>signal_ndim</code> <code>+ 1</code>        dimensions</p></td>
    </tr>
    <tr>
      <th>signal_ndim</th>
      <td><p>(int) the number of dimensions in each signal.        <code>signal_ndim</code> can only be 1, 2 or 3</p></td>
    </tr>
    <tr>
      <th>normalized</th>
      <td><p>(bool, optional) controls whether to return normalized results.        Default: <code>False</code></p></td>
    </tr>
    </table>

    <h2 class="hasAnchor" id="note"><a class="anchor" href="#note"></a>Note</h2>

    
<pre>For CUDA tensors, an LRU cache is used for cuFFT plans to speed up
repeatedly running FFT methods on tensors of same geometry with same
configuration. See cufft-plan-cache for more details on how to
monitor and control the cache.
</pre>

    <h2 class="hasAnchor" id="fft-input-signal-ndim-normalized-false-gt-tensor-"><a class="anchor" href="#fft-input-signal-ndim-normalized-false-gt-tensor-"></a>fft(input, signal_ndim, normalized=False) -&gt; Tensor </h2>

    


<p>Complex-to-complex Discrete Fourier Transform</p>
<p>This method computes the complex-to-complex discrete Fourier transform.
Ignoring the batch dimensions, it computes the following expression:</p>
<p>$$
    X[\omega_1, \dots, \omega_d] =
        \sum_{n_1=0}^{N_1-1} \dots \sum_{n_d=0}^{N_d-1} x[n_1, \dots, n_d]
         e^{-j\ 2 \pi \sum_{i=0}^d \frac{\omega_i n_i}{N_i}},
$$
where \(d\) = <code>signal_ndim</code> is number of dimensions for the
signal, and \(N_i\) is the size of signal dimension \(i\).</p>
<p>This method supports 1D, 2D and 3D complex-to-complex transforms, indicated
by <code>signal_ndim</code>. <code>input</code> must be a tensor with last dimension
of size 2, representing the real and imaginary components of complex
numbers, and should have at least <code>signal_ndim + 1</code> dimensions with optionally
arbitrary number of leading batch dimensions. If <code>normalized</code> is set to
<code>True</code>, this normalizes the result by dividing it with
\(\sqrt{\prod_{i=1}^K N_i}\) so that the operator is unitary.</p>
<p>Returns the real and the imaginary parts together as one tensor of the same
shape of <code>input</code>.</p>
<p>The inverse of this function is <code>~torch.ifft</code>.</p>
    <h2 class="hasAnchor" id="warning"><a class="anchor" href="#warning"></a>Warning</h2>

    

<p>For CPU tensors, this method is currently only available with MKL. Use
<code>torch_backends.mkl.is_available</code> to check if MKL is installed.</p>

    <h2 class="hasAnchor" id="examples"><a class="anchor" href="#examples"></a>Examples</h2>
    <pre class="examples"><div class='input'>
<span class='co'># unbatched 2D FFT</span>
<span class='no'>x</span> <span class='kw'>=</span> <span class='fu'><a href='torch_randn.html'>torch_randn</a></span>(<span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span>(<span class='fl'>4</span>, <span class='fl'>3</span>, <span class='fl'>2</span>))
<span class='fu'>torch_fft</span>(<span class='no'>x</span>, <span class='fl'>2</span>)</div><div class='output co'>#&gt; torch_tensor 
#&gt; (1,.,.) = 
#&gt;   3.7597 -1.1886
#&gt;  -1.9001 -5.6069
#&gt;   1.1265  1.4221
#&gt; 
#&gt; (2,.,.) = 
#&gt;   3.8713  0.8145
#&gt;  -0.5266  1.7461
#&gt;   0.3583 -1.9295
#&gt; 
#&gt; (3,.,.) = 
#&gt;   5.2281 -7.4829
#&gt;   0.0981  1.7069
#&gt;   5.2268 -4.2273
#&gt; 
#&gt; (4,.,.) = 
#&gt;  -0.7460  3.9434
#&gt;   0.6337  3.1282
#&gt;   0.6950 -2.0228
#&gt; [ CPUFloatType{4,3,2} ]</div><div class='input'><span class='co'># batched 1D FFT</span>
<span class='fu'>torch_fft</span>(<span class='no'>x</span>, <span class='fl'>1</span>)</div><div class='output co'>#&gt; torch_tensor 
#&gt; (1,.,.) = 
#&gt;   3.0283 -0.9784
#&gt;  -0.4237  0.2436
#&gt;   1.8517 -1.6894
#&gt; 
#&gt; (2,.,.) = 
#&gt;   0.4151  2.7279
#&gt;  -0.1540 -2.1185
#&gt;  -1.0484  1.3282
#&gt; 
#&gt; (3,.,.) = 
#&gt;   1.4656 -3.3573
#&gt;  -0.4773 -2.1936
#&gt;   1.3250  0.2868
#&gt; 
#&gt; (4,.,.) = 
#&gt;  -1.1493  0.4193
#&gt;  -0.8451 -1.5383
#&gt;  -1.0017  1.4965
#&gt; [ CPUFloatType{4,3,2} ]</div><div class='input'><span class='co'># arbitrary number of batch dimensions, 2D FFT</span>
<span class='no'>x</span> <span class='kw'>=</span> <span class='fu'><a href='torch_randn.html'>torch_randn</a></span>(<span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span>(<span class='fl'>3</span>, <span class='fl'>3</span>, <span class='fl'>5</span>, <span class='fl'>5</span>, <span class='fl'>2</span>))
<span class='fu'>torch_fft</span>(<span class='no'>x</span>, <span class='fl'>2</span>)</div><div class='output co'>#&gt; torch_tensor 
#&gt; (1,1,1,.,.) = 
#&gt;   3.9048  3.1432
#&gt;   0.0906 -1.4782
#&gt;  -3.1744 -0.6598
#&gt;   4.5718  3.0818
#&gt;   0.3719 -1.1976
#&gt; 
#&gt; (2,1,1,.,.) = 
#&gt;   -7.6779   2.9652
#&gt;   -0.1711  -0.2902
#&gt;   -5.0236   1.5802
#&gt;    8.4835   4.0315
#&gt;   -0.5778  13.5156
#&gt; 
#&gt; (3,1,1,.,.) = 
#&gt;   -9.1267   0.3414
#&gt;   12.6732   2.5624
#&gt;   -1.6024  -1.1598
#&gt;   -2.7183   7.7227
#&gt;   10.5616  -1.0794
#&gt; 
#&gt; (1,2,1,.,.) = 
#&gt;  -6.6029  7.6129
#&gt;  -3.0540 -0.2958
#&gt;  -8.3002  5.1049
#&gt;   7.8164  5.5754
#&gt;   3.0946  0.7063
#&gt; 
#&gt; (2,2,1,.,.) = 
#&gt;  -2.8983  2.7208
#&gt;   0.6131 -3.3917
#&gt;  -4.0286 -5.1291
#&gt;   4.0895  2.4884
#&gt;   5.0024  3.4807
#&gt; 
#&gt; (3,2,1,.,.) = 
#&gt;   6.0266 -2.8156
#&gt;   2.3848  5.1107
#&gt;   1.2038 -1.4473
#&gt;  -0.5940 -3.3553
#&gt;   7.7593  4.5381
#&gt; 
#&gt; (1,3,1,.,.) = 
#&gt;   3.0706  4.2322
#&gt;  -5.8536  0.6576
#&gt;   1.8932 -3.3699
#&gt;   6.9478  9.2783
#&gt;  -0.3539  6.4962
#&gt; 
#&gt; (2,3,1,.,.) = 
#&gt;    1.1079   2.9124
#&gt;   -1.6890   5.6916
#&gt;   -6.1432  -9.3719
#&gt;   -0.6300  15.1821
#&gt;    0.7383   4.2963
#&gt; 
#&gt; (3,3,1,.,.) = 
#&gt;   0.2897 -1.2233
#&gt;   1.0957  3.8539
#&gt;  -3.3942  6.6180
#&gt;   4.9179  1.9841
#&gt;   4.3170  8.6111
#&gt; 
#&gt; (1,1,2,.,.) = 
#&gt;   -1.9210  12.4019
#&gt;    5.4754   0.2391
#&gt;   -3.4419   4.0215
#&gt;    3.6538  -1.8343
#&gt;   -0.9536   5.1797
#&gt; 
#&gt; (2,1,2,.,.) = 
#&gt;  -5.7539 -5.6602
#&gt;   0.2156 -3.7421
#&gt;   7.2037 -4.2497
#&gt;  -4.2686 -0.7439
#&gt;  -3.5559 -6.5582
#&gt; 
#&gt; (3,1,2,.,.) = 
#&gt;  -2.1671 -7.5903
#&gt;   6.0292 -0.6595
#&gt;   0.1952 -2.2188
#&gt;  -6.6567 -0.8834
#&gt;  -1.0638  4.1578
#&gt; 
#&gt; (1,2,2,.,.) = 
#&gt;   8.5629  2.7812
#&gt;  -2.8361 -10.1750
#&gt;  -7.0520 -9.2168
#&gt;  -2.4850 -0.4513
#&gt;  -0.6809 -1.1388
#&gt; 
#&gt; (2,2,2,.,.) = 
#&gt;   5.5307  7.4056
#&gt;   0.5267  4.6140
#&gt;   5.0932 -2.7840
#&gt;   3.6697  3.0834
#&gt;  -2.9730  1.8544
#&gt; 
#&gt; (3,2,2,.,.) = 
#&gt;   5.6465 -4.9537
#&gt;   1.1380 -4.0479
#&gt;   2.9763  5.4800
#&gt;   5.3889 -4.4113
#&gt;   3.5672 -1.3771
#&gt; 
#&gt; (1,3,2,.,.) = 
#&gt;  -5.7418  1.5811
#&gt;  -3.9376 -7.6909
#&gt;  -0.8321 -1.0367
#&gt;  -0.2761  0.0475
#&gt;   0.3542 -5.5879
#&gt; 
#&gt; (2,3,2,.,.) = 
#&gt;   1.8804  1.1318
#&gt;   4.9749 -2.1704
#&gt;   4.0903  1.7678
#&gt;  -8.3534  0.4683
#&gt;   7.3973 -6.7424
#&gt; 
#&gt; (3,3,2,.,.) = 
#&gt;   4.9704 -2.0084
#&gt;  -3.1934 -4.6632
#&gt;  -0.2653  6.4793
#&gt;  -6.6119 -1.4872
#&gt;  -4.6379  9.0453
#&gt; 
#&gt; (1,1,3,.,.) = 
#&gt;   8.8293  5.7999
#&gt;   4.0675  0.0310
#&gt;   3.1614 -2.0808
#&gt;   7.2978 -4.6113
#&gt;  -5.3526  2.6249
#&gt; 
#&gt; (2,1,3,.,.) = 
#&gt;   -3.6297  13.0879
#&gt;    0.4426   5.3592
#&gt;   -3.0188  -3.5272
#&gt;   -6.3864  -3.2856
#&gt;    0.1971   4.6184
#&gt; 
#&gt; (3,1,3,.,.) = 
#&gt;   2.3457 -4.5059
#&gt;  -0.6642  2.9940
#&gt;   3.5531 -1.6692
#&gt;  -6.2980 -4.2022
#&gt;   3.8599 -2.0988
#&gt; 
#&gt; (1,2,3,.,.) = 
#&gt;   -7.7148  -0.9027
#&gt;   -0.2000  12.1602
#&gt;    6.0467  -1.3767
#&gt;    3.5084 -11.1105
#&gt;   -5.1683  11.5905
#&gt; 
#&gt; (2,2,3,.,.) = 
#&gt;    1.5637   2.3064
#&gt;    1.0590  12.7400
#&gt;    0.9627   5.5048
#&gt;   -4.8049  -7.6770
#&gt;   -1.3564   2.9993
#&gt; 
#&gt; (3,2,3,.,.) = 
#&gt;   0.0609 -1.4924
#&gt;  -0.3709 -1.4160
#&gt;  -3.0492  0.7079
#&gt;   4.8611  4.9021
#&gt;   3.3353 -10.5338
#&gt; 
#&gt; (1,3,3,.,.) = 
#&gt;  -0.4755 -2.0368
#&gt;   4.7065 -9.7528
#&gt;  -2.7965  0.2146
#&gt;  -1.2834  3.7712
#&gt;   4.4591  6.8290
#&gt; 
#&gt; (2,3,3,.,.) = 
#&gt;  -5.7518 -2.4945
#&gt;   1.8783  3.8928
#&gt;   4.0760  4.7860
#&gt;   2.5946 -0.8541
#&gt;  -1.8343 -5.6057
#&gt; 
#&gt; (3,3,3,.,.) = 
#&gt;  -6.7338  3.6179
#&gt;  -7.4093  2.7052
#&gt;   0.7138  3.8406
#&gt;   9.5513  2.5706
#&gt;  -1.2045  1.0910
#&gt; 
#&gt; (1,1,4,.,.) = 
#&gt;  -3.9263  7.0106
#&gt;  -2.5195  0.4428
#&gt;  -6.4888  6.5006
#&gt;  -1.4293 -4.1111
#&gt;  -1.6695 -6.3649
#&gt; 
#&gt; (2,1,4,.,.) = 
#&gt;  -13.0799  -0.5649
#&gt;   -0.3013   0.2854
#&gt;    0.5151   3.1697
#&gt;    6.3659   0.5913
#&gt;    0.4552   5.0343
#&gt; 
#&gt; (3,1,4,.,.) = 
#&gt;   1.9795 -0.2726
#&gt;  -4.3545 -0.0908
#&gt;  -3.4449  0.3937
#&gt;  -3.4244 -1.4043
#&gt;  -2.4324 -2.5763
#&gt; 
#&gt; (1,2,4,.,.) = 
#&gt;    0.5603   0.6206
#&gt;    5.6226  -2.7528
#&gt;   -2.9207  10.1461
#&gt;   -0.3291  -0.8305
#&gt;    2.9910  -0.7708
#&gt; 
#&gt; (2,2,4,.,.) = 
#&gt;  -1.2602 -2.4204
#&gt;  -0.3227 -2.0235
#&gt;  -3.2672  6.6582
#&gt;  -8.3305  6.8149
#&gt;   3.9845  5.6631
#&gt; 
#&gt; (3,2,4,.,.) = 
#&gt;  -1.2129  9.8860
#&gt;  -1.5130  0.3129
#&gt;  -4.7812  4.0430
#&gt;   0.3661 -4.0057
#&gt;   5.6167 -4.6773
#&gt; 
#&gt; (1,3,4,.,.) = 
#&gt;   0.7721 -3.8790
#&gt;  -0.0949  2.0325
#&gt;   1.8220  0.2131
#&gt;  -6.4614  2.3202
#&gt;  -1.1680  0.2643
#&gt; 
#&gt; (2,3,4,.,.) = 
#&gt;   9.5614 -0.8108
#&gt;   2.7266  1.6703
#&gt;   0.2408 -3.3653
#&gt;  -6.7654  8.0736
#&gt;  -3.9306 -1.7141
#&gt; 
#&gt; (3,3,4,.,.) = 
#&gt;   -3.5046  -0.5573
#&gt;   -1.6020  -2.8473
#&gt;   10.8374  -3.5995
#&gt;    8.5485 -10.3913
#&gt;   -2.6153   5.8670
#&gt; 
#&gt; (1,1,5,.,.) = 
#&gt;  -2.5564 -5.2922
#&gt;   6.6451 -8.2465
#&gt;   0.5204 -2.3425
#&gt;  -4.9790  8.5256
#&gt;  -3.3916  9.6140
#&gt; 
#&gt; (2,1,5,.,.) = 
#&gt;  -5.4102  5.2655
#&gt;  -1.6151  3.1989
#&gt;   0.0230  2.6894
#&gt;   0.2610  3.8089
#&gt;   7.2187  4.4831
#&gt; 
#&gt; (3,1,5,.,.) = 
#&gt;  -2.1687  1.1452
#&gt;   7.2405  0.0237
#&gt;  -6.2894 -0.4475
#&gt;   6.4199 -2.1653
#&gt;  -0.8690 -0.6281
#&gt; 
#&gt; (1,2,5,.,.) = 
#&gt;   2.3866 -9.5212
#&gt;   1.0424 -0.3695
#&gt;   4.0655 -6.4998
#&gt;   1.7199 -1.8309
#&gt;  -2.6886  1.4756
#&gt; 
#&gt; (2,2,5,.,.) = 
#&gt;   -0.8965   3.9333
#&gt;    2.8927   1.2791
#&gt;    6.1394  -3.6822
#&gt;   -2.0155   7.1541
#&gt;   10.7753   1.6398
#&gt; 
#&gt; (3,2,5,.,.) = 
#&gt;  -4.9257  3.2729
#&gt;   1.1625  2.1176
#&gt;  -2.9741  5.9234
#&gt;   2.4396 -3.4627
#&gt;   0.0933 -3.2734
#&gt; 
#&gt; (1,3,5,.,.) = 
#&gt;    0.1165  -6.3275
#&gt;   -0.6939   2.6154
#&gt;   -2.4516  -4.5182
#&gt;    3.1925  -9.0616
#&gt;    1.5353  12.6257
#&gt; 
#&gt; (2,3,5,.,.) = 
#&gt;  -2.7058 -0.5135
#&gt;   0.3806 -0.7217
#&gt;  -2.0976 -8.6706
#&gt;  -3.5017 -3.3308
#&gt;   0.7527 -5.8317
#&gt; 
#&gt; (3,3,5,.,.) = 
#&gt;    3.2427   5.2477
#&gt;   -1.4603   2.0772
#&gt;    4.5818  -0.3293
#&gt;   -3.7817  -1.5063
#&gt;   11.3401  -4.1142
#&gt; [ CPUFloatType{3,3,5,5,2} ]</div><div class='input'>
</div></pre>
  </div>
  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
    <h2>Contents</h2>
    <ul class="nav nav-pills nav-stacked">
      <li><a href="#arguments">Arguments</a></li>
      <li><a href="#note">Note</a></li>
      <li><a href="#fft-input-signal-ndim-normalized-false-gt-tensor-">fft(input, signal_ndim, normalized=False) -&gt; Tensor </a></li>
      <li><a href="#warning">Warning</a></li>
      <li><a href="#examples">Examples</a></li>
    </ul>

  </div>
</div>


      <footer>
      <div class="copyright">
  <p>Developed by Daniel Falbel.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="https://pkgdown.r-lib.org/">pkgdown</a> 1.4.1.9000.</p>
</div>

      </footer>
   </div>

  


  </body>
</html>


