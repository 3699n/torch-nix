<!-- Generated by pkgdown: do not edit by hand -->
<!DOCTYPE html>
<html lang="en">
  <head>
  <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<title>Matmul — torch_matmul • torchr</title>


<!-- jquery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js" integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin="anonymous"></script>
<!-- Bootstrap -->
<link href="https://cdnjs.cloudflare.com/ajax/libs/bootswatch/3.3.7/united/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous" />


<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha256-U5ZEeKfGNOja007MMD3YBI0A3OSZOQbeG6z2f2Y0hu8=" crossorigin="anonymous"></script>

<!-- Font Awesome icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.7.1/css/all.min.css" integrity="sha256-nAmazAk6vS34Xqo0BSrTb+abbtFlgsFK7NKSi6o7Y78=" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.7.1/css/v4-shims.min.css" integrity="sha256-6qHlizsOWFskGlwVOKuns+D1nB6ssZrHQrNj1wGplHc=" crossorigin="anonymous" />

<!-- clipboard.js -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.4/clipboard.min.js" integrity="sha256-FiZwavyI2V6+EXO1U+xzLG3IKldpiTFf3153ea9zikQ=" crossorigin="anonymous"></script>

<!-- headroom.js -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.9.4/headroom.min.js" integrity="sha256-DJFC1kqIhelURkuza0AvYal5RxMtpzLjFhsnVIeuk+U=" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.9.4/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script>

<!-- pkgdown -->
<link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script>




<meta property="og:title" content="Matmul — torch_matmul" />
<meta property="og:description" content="Matmul" />
<meta name="twitter:card" content="summary" />




<!-- mathjax -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script>

<!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->



  </head>

  <body>
    <div class="container template-reference-topic">
      <header>
      <div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">torchr</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="Released version">0.0.1.9000</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="../index.html">
    <span class="fas fa fas fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="../articles/extending-autograd.html">Extending Autograd</a>
    </li>
    <li>
      <a href="../articles/indexing.html">Indexing tensors</a>
    </li>
    <li>
      <a href="../articles/using-autograd.html">Using autograd</a>
    </li>
  </ul>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://github.com/mlverse/torch">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
      
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

      

      </header>

<div class="row">
  <div class="col-md-9 contents">
    <div class="page-header">
    <h1>Matmul</h1>
    <small class="dont-index">Source: <a href='https://github.com/mlverse/torch/blob/master/R/gen-namespace-docs.R'><code>R/gen-namespace-docs.R</code></a>, <a href='https://github.com/mlverse/torch/blob/master/R/gen-namespace-examples.R'><code>R/gen-namespace-examples.R</code></a></small>
    <div class="hidden name"><code>torch_matmul.Rd</code></div>
    </div>

    <div class="ref-description">
    <p>Matmul</p>
    </div>


    <h2 class="hasAnchor" id="arguments"><a class="anchor" href="#arguments"></a>Arguments</h2>
    <table class="ref-arguments">
    <colgroup><col class="name" /><col class="desc" /></colgroup>
    <tr>
      <th>input</th>
      <td><p>(Tensor) the first tensor to be multiplied</p></td>
    </tr>
    <tr>
      <th>other</th>
      <td><p>(Tensor) the second tensor to be multiplied</p></td>
    </tr>
    <tr>
      <th>out</th>
      <td><p>(Tensor, optional) the output tensor.</p></td>
    </tr>
    </table>

    <h2 class="hasAnchor" id="note"><a class="anchor" href="#note"></a>Note</h2>

    
<pre>The 1-dimensional dot product version of this function does not support an `out` parameter.
</pre>

    <h2 class="hasAnchor" id="matmul-input-other-out-none-gt-tensor-"><a class="anchor" href="#matmul-input-other-out-none-gt-tensor-"></a>matmul(input, other, out=None) -&gt; Tensor </h2>

    


<p>Matrix product of two tensors.</p>
<p>The behavior depends on the dimensionality of the tensors as follows:</p><ul>
<li><p>If both tensors are 1-dimensional, the dot product (scalar) is returned.</p></li>
<li><p>If both arguments are 2-dimensional, the matrix-matrix product is returned.</p></li>
<li><p>If the first argument is 1-dimensional and the second argument is 2-dimensional,
a 1 is prepended to its dimension for the purpose of the matrix multiply.
After the matrix multiply, the prepended dimension is removed.</p></li>
<li><p>If the first argument is 2-dimensional and the second argument is 1-dimensional,
the matrix-vector product is returned.</p></li>
<li><p>If both arguments are at least 1-dimensional and at least one argument is
N-dimensional (where N &gt; 2), then a batched matrix multiply is returned.  If the first
argument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the
batched matrix multiply and removed after.  If the second argument is 1-dimensional, a
1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.
The non-matrix (i.e. batch) dimensions are broadcasted  (and thus
must be broadcastable).  For example, if <code>input</code> is a
\((j \times 1 \times n \times m)\) tensor and <code>other</code> is a \((k \times m \times p)\)
tensor, <code>out</code> will be an \((j \times k \times n \times p)\) tensor.</p></li>
</ul>


    <h2 class="hasAnchor" id="examples"><a class="anchor" href="#examples"></a>Examples</h2>
    <pre class="examples"><div class='input'>
<span class='co'># vector x vector</span>
<span class='no'>tensor1</span> <span class='kw'>=</span> <span class='fu'><a href='torch_randn.html'>torch_randn</a></span>(<span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span>(<span class='fl'>3</span>))
<span class='no'>tensor2</span> <span class='kw'>=</span> <span class='fu'><a href='torch_randn.html'>torch_randn</a></span>(<span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span>(<span class='fl'>3</span>))
<span class='fu'>torch_matmul</span>(<span class='no'>tensor1</span>, <span class='no'>tensor2</span>)</div><div class='output co'>#&gt; torch_tensor 
#&gt; -1.5543
#&gt; [ CPUFloatType{} ]</div><div class='input'><span class='co'># matrix x vector</span>
<span class='no'>tensor1</span> <span class='kw'>=</span> <span class='fu'><a href='torch_randn.html'>torch_randn</a></span>(<span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span>(<span class='fl'>3</span>, <span class='fl'>4</span>))
<span class='no'>tensor2</span> <span class='kw'>=</span> <span class='fu'><a href='torch_randn.html'>torch_randn</a></span>(<span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span>(<span class='fl'>4</span>))
<span class='fu'>torch_matmul</span>(<span class='no'>tensor1</span>, <span class='no'>tensor2</span>)</div><div class='output co'>#&gt; torch_tensor 
#&gt; -1.0748
#&gt;  0.0295
#&gt; -1.3756
#&gt; [ CPUFloatType{3} ]</div><div class='input'><span class='co'># batched matrix x broadcasted vector</span>
<span class='no'>tensor1</span> <span class='kw'>=</span> <span class='fu'><a href='torch_randn.html'>torch_randn</a></span>(<span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span>(<span class='fl'>10</span>, <span class='fl'>3</span>, <span class='fl'>4</span>))
<span class='no'>tensor2</span> <span class='kw'>=</span> <span class='fu'><a href='torch_randn.html'>torch_randn</a></span>(<span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span>(<span class='fl'>4</span>))
<span class='fu'>torch_matmul</span>(<span class='no'>tensor1</span>, <span class='no'>tensor2</span>)</div><div class='output co'>#&gt; torch_tensor 
#&gt;  0.5941  0.8701 -0.2910
#&gt;  0.4488  0.0754  1.3375
#&gt; -1.2592  1.8409  0.5066
#&gt;  0.6234 -0.2534  0.8752
#&gt; -0.5142  1.2403  1.7472
#&gt; -1.2152  0.1999  0.2109
#&gt;  1.0808 -0.0185  0.0017
#&gt;  0.1728  0.9394 -1.1047
#&gt;  0.6742  0.0913  1.2810
#&gt; -0.0093 -0.2898  0.8128
#&gt; [ CPUFloatType{10,3} ]</div><div class='input'><span class='co'># batched matrix x batched matrix</span>
<span class='no'>tensor1</span> <span class='kw'>=</span> <span class='fu'><a href='torch_randn.html'>torch_randn</a></span>(<span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span>(<span class='fl'>10</span>, <span class='fl'>3</span>, <span class='fl'>4</span>))
<span class='no'>tensor2</span> <span class='kw'>=</span> <span class='fu'><a href='torch_randn.html'>torch_randn</a></span>(<span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span>(<span class='fl'>10</span>, <span class='fl'>4</span>, <span class='fl'>5</span>))
<span class='fu'>torch_matmul</span>(<span class='no'>tensor1</span>, <span class='no'>tensor2</span>)</div><div class='output co'>#&gt; torch_tensor 
#&gt; (1,.,.) = 
#&gt;   -4.9010   1.2752  -1.9103  -0.7308  -1.6892
#&gt;   10.0931  -0.5024   1.8872   3.2587  -0.4530
#&gt;   -3.1845  -0.3745  -2.1631   0.8955   0.9524
#&gt; 
#&gt; (2,.,.) = 
#&gt;  -0.0195  0.2336 -0.1648 -1.1423 -0.1834
#&gt;  -0.0492 -1.1049 -0.4440 -1.5038  0.6879
#&gt;   0.3193  1.6756 -0.6901 -0.7054  0.0836
#&gt; 
#&gt; (3,.,.) = 
#&gt;   2.9058 -4.0333 -4.1481  2.0654  0.7059
#&gt;  -4.0311  7.1441  0.9818 -4.7148  0.7912
#&gt;  -2.1133  2.4464  0.9383  2.0209 -0.3702
#&gt; 
#&gt; (4,.,.) = 
#&gt;   2.8855  0.6784 -3.8542  0.4534  2.8369
#&gt;   3.8723 -0.1521 -0.2855  1.6627  4.4765
#&gt;  -1.9402 -0.1551  2.6832 -1.0236 -0.1508
#&gt; 
#&gt; (5,.,.) = 
#&gt;  -0.8675 -1.0538  1.7580 -0.8748  0.2218
#&gt;   1.0755  0.5115  2.3009 -3.2440  2.9478
#&gt;  -1.5155 -1.2206 -1.6970  2.6380 -2.9842
#&gt; 
#&gt; (6,.,.) = 
#&gt;   0.7153  2.8359 -0.4342 -1.5048 -1.6542
#&gt;   0.3879  2.5230 -0.1644 -1.5833 -1.5019
#&gt;   0.7879  2.5858  0.6381 -0.8508  0.2870
#&gt; 
#&gt; (7,.,.) = 
#&gt;   0.7594  0.4043 -2.7816  1.1153 -1.0090
#&gt;   3.1190 -1.0716 -2.6517  1.6220 -3.4661
#&gt;  -4.6712  2.7996 -4.1126  0.8684  4.4905
#&gt; 
#&gt; (8,.,.) = 
#&gt;  -0.8368  0.7409  1.2285 -0.0344 -0.1216
#&gt;  -2.5213  1.4233  5.0653  0.2900 -2.7408
#&gt;  -1.2304 -0.8387  0.9760 -0.1214 -0.2982
#&gt; 
#&gt; (9,.,.) = 
#&gt;   0.6302  1.9668  2.1260 -3.5661  3.1188
#&gt;  -0.6115 -0.9723 -2.1853  1.5044 -2.4648
#&gt;   0.2309 -0.6932 -1.0314  1.0734 -0.9127
#&gt; 
#&gt; (10,.,.) = 
#&gt;  -0.6085 -1.4543  0.9416 -0.9075  1.1657
#&gt;  -0.1576  2.4780 -1.1478  0.0162 -2.6088
#&gt;   1.8548  1.8830  0.3853  1.0931 -1.1246
#&gt; [ CPUFloatType{10,3,5} ]</div><div class='input'><span class='co'># batched matrix x broadcasted matrix</span>
<span class='no'>tensor1</span> <span class='kw'>=</span> <span class='fu'><a href='torch_randn.html'>torch_randn</a></span>(<span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span>(<span class='fl'>10</span>, <span class='fl'>3</span>, <span class='fl'>4</span>))
<span class='no'>tensor2</span> <span class='kw'>=</span> <span class='fu'><a href='torch_randn.html'>torch_randn</a></span>(<span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span>(<span class='fl'>4</span>, <span class='fl'>5</span>))
<span class='fu'>torch_matmul</span>(<span class='no'>tensor1</span>, <span class='no'>tensor2</span>)</div><div class='output co'>#&gt; torch_tensor 
#&gt; (1,.,.) = 
#&gt;   3.4214 -1.2106 -0.6298  4.1462 -1.6306
#&gt;   1.2793  0.1481 -1.0392  2.2621 -2.3936
#&gt;  -2.2713 -0.2334 -2.1545  2.4741  0.3863
#&gt; 
#&gt; (2,.,.) = 
#&gt;  -0.8221 -1.3350  1.8479 -2.9218  5.1041
#&gt;   0.2587 -1.2589 -0.8096  1.1778  1.8011
#&gt;   0.5955  0.5769  1.6347  0.1286 -1.2819
#&gt; 
#&gt; (3,.,.) = 
#&gt;  -0.7440 -0.2037 -0.7547 -1.5726  1.4278
#&gt;   1.7038 -0.6690  2.0003  0.7086  0.4786
#&gt;  -1.8517 -0.1515  0.1199 -1.2977  2.1332
#&gt; 
#&gt; (4,.,.) = 
#&gt;   1.9669  0.5846  4.5887 -2.0836 -0.5198
#&gt;   2.3897  0.2687 -2.0213  3.4715 -4.1437
#&gt;  -2.2119 -1.0653 -8.9694  9.0768 -2.4080
#&gt; 
#&gt; (5,.,.) = 
#&gt;   1.7037  0.4159  1.0114  0.6847 -2.0591
#&gt;  -2.8276 -0.3166 -1.5908 -0.4944  2.3620
#&gt;   2.6359 -0.6674  1.1492  1.8629 -0.8457
#&gt; 
#&gt; (6,.,.) = 
#&gt;   1.7514  0.3400  1.1350  0.3517 -1.7532
#&gt;  -3.6329 -1.0829  0.8851 -1.8361  5.7042
#&gt;   0.0827  0.5083  0.8891 -4.8145  1.1828
#&gt; 
#&gt; (7,.,.) = 
#&gt;   2.3415  0.4653  3.3203 -1.8986 -0.8973
#&gt;  -0.6047  0.7703 -0.4558 -0.8566 -0.9788
#&gt;  -2.0909  0.4261 -0.0260 -3.4066  1.9474
#&gt; 
#&gt; (8,.,.) = 
#&gt;  -1.6201 -1.0116 -4.6657  5.1869 -0.1742
#&gt;   0.9228 -0.4858  0.9489  1.1449  0.1569
#&gt;  -3.1509  0.2783  2.3634 -6.6384  4.9453
#&gt; 
#&gt; (9,.,.) = 
#&gt;   2.6801  0.1688 -2.3664  4.4283 -4.6229
#&gt;  -0.6608 -0.1312  1.5208 -0.6322  1.3551
#&gt;  -0.5575  1.1899 -1.7771  0.4593 -2.7964
#&gt; 
#&gt; (10,.,.) = 
#&gt;   0.9173 -0.4096  0.9089 -1.1496  0.9921
#&gt;  -0.1213  0.2654 -0.4653  1.7980 -1.3837
#&gt;  -0.4864 -1.1666 -1.8596  3.4988  0.8258
#&gt; [ CPUFloatType{10,3,5} ]</div></pre>
  </div>
  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
    <h2>Contents</h2>
    <ul class="nav nav-pills nav-stacked">
      <li><a href="#arguments">Arguments</a></li>
      <li><a href="#note">Note</a></li>
      <li><a href="#matmul-input-other-out-none-gt-tensor-">matmul(input, other, out=None) -&gt; Tensor </a></li>
      <li><a href="#examples">Examples</a></li>
    </ul>

  </div>
</div>


      <footer>
      <div class="copyright">
  <p>Developed by Daniel Falbel.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="https://pkgdown.r-lib.org/">pkgdown</a> 1.4.1.9000.</p>
</div>

      </footer>
   </div>

  


  </body>
</html>


