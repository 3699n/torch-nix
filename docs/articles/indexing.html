<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Indexing tensors • torch</title>
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js" integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://cdnjs.cloudflare.com/ajax/libs/bootswatch/3.3.7/united/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha256-U5ZEeKfGNOja007MMD3YBI0A3OSZOQbeG6z2f2Y0hu8=" crossorigin="anonymous"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.7.1/css/all.min.css" integrity="sha256-nAmazAk6vS34Xqo0BSrTb+abbtFlgsFK7NKSi6o7Y78=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.7.1/css/v4-shims.min.css" integrity="sha256-6qHlizsOWFskGlwVOKuns+D1nB6ssZrHQrNj1wGplHc=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.4/clipboard.min.js" integrity="sha256-FiZwavyI2V6+EXO1U+xzLG3IKldpiTFf3153ea9zikQ=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.9.4/headroom.min.js" integrity="sha256-DJFC1kqIhelURkuza0AvYal5RxMtpzLjFhsnVIeuk+U=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.9.4/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><meta property="og:title" content="Indexing tensors">
<meta property="og:description" content="">
<meta name="twitter:card" content="summary">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">torch</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="Released version">0.0.1.9000</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../index.html">
    <span class="fas fa fas fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/extending-autograd.html">Extending Autograd</a>
    </li>
    <li>
      <a href="../articles/indexing.html">Indexing tensors</a>
    </li>
    <li>
      <a href="../articles/using-autograd.html">Using autograd</a>
    </li>
  </ul>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/mlverse/torch">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><script src="indexing_files/header-attrs-2.1.1/header-attrs.js"></script><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1>Indexing tensors</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/mlverse/torch/blob/master/vignettes/indexing.Rmd"><code>vignettes/indexing.Rmd</code></a></small>
      <div class="hidden name"><code>indexing.Rmd</code></div>

    </div>

    
    
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1"></a><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span>(torch)</span></code></pre></div>
<p>In this article we describe the indexing operator for torch tensors and how it compares to the R indexing operator for arrays.</p>
<p>Torch’s indexing semantics are closer to numpy’s semantics than R’s. You will find a lot of similarities between this article and the <code>numpy</code> indexing article available <a href="https://docs.scipy.org/doc/numpy-1.10.0/user/basics.indexing.html">here</a>.</p>
<div id="single-element-indexing" class="section level2">
<h2 class="hasAnchor">
<a href="#single-element-indexing" class="anchor"></a>Single element indexing</h2>
<p>Single element indexing for a 1-D tensors is what one expects. It is 1-based, and, unlike R, accepts negative indices for indexing from the end of the array.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1"></a>x &lt;-<span class="st"> </span><span class="kw">torch_tensor</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>)</span>
<span id="cb2-2"><a href="#cb2-2"></a>x[<span class="dv">1</span>]</span>
<span id="cb2-3"><a href="#cb2-3"></a><span class="co">#&gt; torch_tensor </span></span>
<span id="cb2-4"><a href="#cb2-4"></a><span class="co">#&gt; 1</span></span>
<span id="cb2-5"><a href="#cb2-5"></a><span class="co">#&gt; [ CPUIntType{} ]</span></span>
<span id="cb2-6"><a href="#cb2-6"></a>x[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb2-7"><a href="#cb2-7"></a><span class="co">#&gt; torch_tensor </span></span>
<span id="cb2-8"><a href="#cb2-8"></a><span class="co">#&gt; 10</span></span>
<span id="cb2-9"><a href="#cb2-9"></a><span class="co">#&gt; [ CPUIntType{} ]</span></span></code></pre></div>
<p>You can also subset matrices and higher dimensions arrays using the same syntax:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1"></a>x &lt;-<span class="st"> </span>x<span class="op">$</span><span class="kw"><a href="https://rdrr.io/r/stats/reshape.html">reshape</a></span>(<span class="dt">shape =</span> <span class="kw"><a href="https://rdrr.io/r/base/c.html">c</a></span>(<span class="dv">2</span>,<span class="dv">5</span>))</span>
<span id="cb3-2"><a href="#cb3-2"></a>x</span>
<span id="cb3-3"><a href="#cb3-3"></a><span class="co">#&gt; torch_tensor </span></span>
<span id="cb3-4"><a href="#cb3-4"></a><span class="co">#&gt;   1   2   3   4   5</span></span>
<span id="cb3-5"><a href="#cb3-5"></a><span class="co">#&gt;   6   7   8   9  10</span></span>
<span id="cb3-6"><a href="#cb3-6"></a><span class="co">#&gt; [ CPUIntType{2,5} ]</span></span>
<span id="cb3-7"><a href="#cb3-7"></a>x[<span class="dv">1</span>,<span class="dv">3</span>]</span>
<span id="cb3-8"><a href="#cb3-8"></a><span class="co">#&gt; torch_tensor </span></span>
<span id="cb3-9"><a href="#cb3-9"></a><span class="co">#&gt; 3</span></span>
<span id="cb3-10"><a href="#cb3-10"></a><span class="co">#&gt; [ CPUIntType{} ]</span></span>
<span id="cb3-11"><a href="#cb3-11"></a>x[<span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb3-12"><a href="#cb3-12"></a><span class="co">#&gt; torch_tensor </span></span>
<span id="cb3-13"><a href="#cb3-13"></a><span class="co">#&gt; 5</span></span>
<span id="cb3-14"><a href="#cb3-14"></a><span class="co">#&gt; [ CPUIntType{} ]</span></span></code></pre></div>
<p>Note that if one indexes a multidimensional tensor with fewer indices than dimensions, one gets an error, unlike R, that would flatten the array. For example:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1"></a>x[<span class="dv">1</span>]</span>
<span id="cb4-2"><a href="#cb4-2"></a><span class="co">#&gt; Error in `[.torch_tensor`(x, 1): incorrect number of dimensions. Specified 1 should be 2.</span></span></code></pre></div>
</div>
<div id="other-indexing-options" class="section level2">
<h2 class="hasAnchor">
<a href="#other-indexing-options" class="anchor"></a>Other indexing options</h2>
<p>It is possible to slice and stride arrays to extract arrays of the same number of dimensions, but of different sizes than the original. A few examples illustrates best:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1"></a>x &lt;-<span class="st"> </span><span class="kw">torch_tensor</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>)</span>
<span id="cb5-2"><a href="#cb5-2"></a>x</span>
<span id="cb5-3"><a href="#cb5-3"></a><span class="co">#&gt; torch_tensor </span></span>
<span id="cb5-4"><a href="#cb5-4"></a><span class="co">#&gt;   1</span></span>
<span id="cb5-5"><a href="#cb5-5"></a><span class="co">#&gt;   2</span></span>
<span id="cb5-6"><a href="#cb5-6"></a><span class="co">#&gt;   3</span></span>
<span id="cb5-7"><a href="#cb5-7"></a><span class="co">#&gt;   4</span></span>
<span id="cb5-8"><a href="#cb5-8"></a><span class="co">#&gt;   5</span></span>
<span id="cb5-9"><a href="#cb5-9"></a><span class="co">#&gt;   6</span></span>
<span id="cb5-10"><a href="#cb5-10"></a><span class="co">#&gt;   7</span></span>
<span id="cb5-11"><a href="#cb5-11"></a><span class="co">#&gt;   8</span></span>
<span id="cb5-12"><a href="#cb5-12"></a><span class="co">#&gt;   9</span></span>
<span id="cb5-13"><a href="#cb5-13"></a><span class="co">#&gt;  10</span></span>
<span id="cb5-14"><a href="#cb5-14"></a><span class="co">#&gt; [ CPUIntType{10} ]</span></span>
<span id="cb5-15"><a href="#cb5-15"></a>x[<span class="dv">2</span><span class="op">:</span><span class="dv">5</span>]</span>
<span id="cb5-16"><a href="#cb5-16"></a><span class="co">#&gt; torch_tensor </span></span>
<span id="cb5-17"><a href="#cb5-17"></a><span class="co">#&gt;  2</span></span>
<span id="cb5-18"><a href="#cb5-18"></a><span class="co">#&gt;  3</span></span>
<span id="cb5-19"><a href="#cb5-19"></a><span class="co">#&gt;  4</span></span>
<span id="cb5-20"><a href="#cb5-20"></a><span class="co">#&gt;  5</span></span>
<span id="cb5-21"><a href="#cb5-21"></a><span class="co">#&gt; [ CPUIntType{4} ]</span></span>
<span id="cb5-22"><a href="#cb5-22"></a>x[<span class="dv">1</span><span class="op">:</span>(<span class="op">-</span><span class="dv">7</span>)]</span>
<span id="cb5-23"><a href="#cb5-23"></a><span class="co">#&gt; torch_tensor </span></span>
<span id="cb5-24"><a href="#cb5-24"></a><span class="co">#&gt;  1</span></span>
<span id="cb5-25"><a href="#cb5-25"></a><span class="co">#&gt;  2</span></span>
<span id="cb5-26"><a href="#cb5-26"></a><span class="co">#&gt;  3</span></span>
<span id="cb5-27"><a href="#cb5-27"></a><span class="co">#&gt; [ CPUIntType{3} ]</span></span></code></pre></div>
<p>You can also use the <code>1:10:2</code> syntax which means 1 to 10 by 2. For example:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1"></a>x[<span class="dv">1</span><span class="op">:</span><span class="dv">5</span><span class="op">:</span><span class="dv">2</span>]</span>
<span id="cb6-2"><a href="#cb6-2"></a><span class="co">#&gt; torch_tensor </span></span>
<span id="cb6-3"><a href="#cb6-3"></a><span class="co">#&gt;  1</span></span>
<span id="cb6-4"><a href="#cb6-4"></a><span class="co">#&gt;  3</span></span>
<span id="cb6-5"><a href="#cb6-5"></a><span class="co">#&gt;  5</span></span>
<span id="cb6-6"><a href="#cb6-6"></a><span class="co">#&gt; [ CPUIntType{3} ]</span></span></code></pre></div>
<p>Another special syntax is the <code>N</code>, meaning the size of the specified dimension.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1"></a>x[<span class="dv">5</span><span class="op">:</span>N]</span>
<span id="cb7-2"><a href="#cb7-2"></a><span class="co">#&gt; torch_tensor </span></span>
<span id="cb7-3"><a href="#cb7-3"></a><span class="co">#&gt;   5</span></span>
<span id="cb7-4"><a href="#cb7-4"></a><span class="co">#&gt;   6</span></span>
<span id="cb7-5"><a href="#cb7-5"></a><span class="co">#&gt;   7</span></span>
<span id="cb7-6"><a href="#cb7-6"></a><span class="co">#&gt;   8</span></span>
<span id="cb7-7"><a href="#cb7-7"></a><span class="co">#&gt;   9</span></span>
<span id="cb7-8"><a href="#cb7-8"></a><span class="co">#&gt;  10</span></span>
<span id="cb7-9"><a href="#cb7-9"></a><span class="co">#&gt; [ CPUIntType{6} ]</span></span></code></pre></div>
</div>
<div id="dealing-with-variable-number-of-indices" class="section level2">
<h2 class="hasAnchor">
<a href="#dealing-with-variable-number-of-indices" class="anchor"></a>Dealing with variable number of indices</h2>
<p>The index syntax is very powerful but limiting when dealing with a variable number of indices. For example, if you want to write a function that can handle arguments with various numbers of dimensions without having to write special case code for each number of possible dimensions, how can that be done? If one supplies to the index a tuple, the tuple will be interpreted as a list of indices. For example:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1"></a>z &lt;-<span class="st"> </span><span class="kw">torch_tensor</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">125</span>)<span class="op">$</span><span class="kw"><a href="https://rdrr.io/r/stats/reshape.html">reshape</a></span>(<span class="kw"><a href="https://rdrr.io/r/base/c.html">c</a></span>(<span class="dv">5</span>,<span class="dv">5</span>,<span class="dv">5</span>))</span></code></pre></div>
<p>If you want to always subset the first or last dimension you can use the <code>..</code>:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1"></a>z[<span class="dv">1</span>,..]</span>
<span id="cb9-2"><a href="#cb9-2"></a><span class="co">#&gt; torch_tensor </span></span>
<span id="cb9-3"><a href="#cb9-3"></a><span class="co">#&gt;   1   2   3   4   5</span></span>
<span id="cb9-4"><a href="#cb9-4"></a><span class="co">#&gt;   6   7   8   9  10</span></span>
<span id="cb9-5"><a href="#cb9-5"></a><span class="co">#&gt;  11  12  13  14  15</span></span>
<span id="cb9-6"><a href="#cb9-6"></a><span class="co">#&gt;  16  17  18  19  20</span></span>
<span id="cb9-7"><a href="#cb9-7"></a><span class="co">#&gt;  21  22  23  24  25</span></span>
<span id="cb9-8"><a href="#cb9-8"></a><span class="co">#&gt; [ CPUIntType{5,5} ]</span></span>
<span id="cb9-9"><a href="#cb9-9"></a>z[..,<span class="dv">1</span>]</span>
<span id="cb9-10"><a href="#cb9-10"></a><span class="co">#&gt; torch_tensor </span></span>
<span id="cb9-11"><a href="#cb9-11"></a><span class="co">#&gt;    1    6   11   16   21</span></span>
<span id="cb9-12"><a href="#cb9-12"></a><span class="co">#&gt;   26   31   36   41   46</span></span>
<span id="cb9-13"><a href="#cb9-13"></a><span class="co">#&gt;   51   56   61   66   71</span></span>
<span id="cb9-14"><a href="#cb9-14"></a><span class="co">#&gt;   76   81   86   91   96</span></span>
<span id="cb9-15"><a href="#cb9-15"></a><span class="co">#&gt;  101  106  111  116  121</span></span>
<span id="cb9-16"><a href="#cb9-16"></a><span class="co">#&gt; [ CPUIntType{5,5} ]</span></span></code></pre></div>
<p>You can also use the unsplice operator here, for example:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1"></a>i &lt;-<span class="st"> </span><span class="kw"><a href="https://rdrr.io/r/base/c.html">c</a></span>(<span class="dv">5</span>,<span class="dv">5</span>,<span class="dv">5</span>)</span>
<span id="cb10-2"><a href="#cb10-2"></a>z[<span class="op">!!!</span>i]</span>
<span id="cb10-3"><a href="#cb10-3"></a><span class="co">#&gt; torch_tensor </span></span>
<span id="cb10-4"><a href="#cb10-4"></a><span class="co">#&gt; 125</span></span>
<span id="cb10-5"><a href="#cb10-5"></a><span class="co">#&gt; [ CPUIntType{} ]</span></span></code></pre></div>
<p>More complex indexing should make use of <code>torch_select</code> or <code>torch_slice</code>.</p>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">

        <div id="tocnav">
      <h2 class="hasAnchor">
<a href="#tocnav" class="anchor"></a>Contents</h2>
      <ul class="nav nav-pills nav-stacked">
<li><a href="#single-element-indexing">Single element indexing</a></li>
      <li><a href="#other-indexing-options">Other indexing options</a></li>
      <li><a href="#dealing-with-variable-number-of-indices">Dealing with variable number of indices</a></li>
      </ul>
</div>
      </div>

</div>



      <footer><div class="copyright">
  <p>Developed by Daniel Falbel, Javier Luraschi.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="https://pkgdown.r-lib.org/">pkgdown</a> 1.4.1.9000.</p>
</div>

      </footer>
</div>

  


  </body>
</html>
