msgid ""
msgstr ""
"Project-Id-Version: torch 0.11.0.9002\n"
"POT-Creation-Date: 2023-10-09 00:47+0200\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"Language: \n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=CHARSET\n"
"Content-Transfer-Encoding: 8bit\n"

#: R7.R:46
msgid "can only set to public, private and active"
msgstr ""

#: autograd.R:34
msgid ""
"This mode should be enabled only for debugging as the different tests will "
"slow down your program execution."
msgstr ""

#: autograd.R:132
msgid "`keep_graph` has been deprecated. Please use `retain_graph` instead."
msgstr ""

#: autograd.R:133
msgid "i"
msgstr ""

#: autograd.R:133
msgid "`keep_graph` will take precedence."
msgstr ""

#: autograd.R:134
msgid "once"
msgstr ""

#: autograd.R:135
msgid "keep_graph"
msgstr ""

#: call_torch_function.R:39
msgid ""
"Because this function allows access to unexported functions, please use with "
"caution, and\n"
"            only if you are sure know what you are doing. Unexported "
"functions will expect inputs that\n"
"            are more C++-like than R-like. For example, they will expect all "
"indexes to be 0-based instead\n"
"            of 1-based. In addition unexported functions may be subject to "
"removal from the API without\n"
"            warning. Set quiet = TRUE to silence this warning."
msgstr ""

#: call_torch_function.R:47
msgid "Only functions prefixed with 'torch_' are available from this function."
msgstr ""

#: call_torch_function.R:54
msgid ""
"A function of name %s was not found. Please check your spelling and that the "
"desired function exists."
msgstr ""

#: codegen-utils.R:5
msgid "Dimension is 1-based, but found 0."
msgstr ""

#: codegen-utils.R:40
msgid "{fun_name} does not exist"
msgstr ""

#: conditions.R:18
msgid "warning"
msgstr ""

#: conditions.R:28
msgid "deprecated"
msgstr ""

#: creation-ops.R:6 creation-ops.R:12
msgid "You should specify a single size argument."
msgstr ""

#: creation-ops.R:260
msgid "This function is deprecated in favor of torch_arange."
msgstr ""

#: creation-ops.R:380
msgid "values must be length 1"
msgstr ""

#: creation-ops.R:384
msgid "value must be a length 1 vector"
msgstr ""

#: cuda.R:42
msgid "device must be an integer between 0 and the number of devices minus 1"
msgstr ""

#: device.R:12
msgid "type should not include an index because index was passed explicitly"
msgstr ""

#: device.R:110
msgid "y is not a torch_device"
msgstr ""

#: distributions-bernoulli.R:21 distributions-categorical.R:18
msgid "Either `probs` or `logits` must be specified, but not both."
msgstr ""

#: distributions-categorical.R:23
msgid "`probs` must be at least one-dimensional."
msgstr ""

#: distributions-categorical.R:29
msgid "`logits` must be at least one-dimensional."
msgstr ""

#: distributions-constraints.R:38
msgid "Cannot determine validity of dependent constraint"
msgstr ""

#: distributions-constraints.R:60
msgid "Expected value$dim() >= {expected} but got {value$dim()}"
msgstr ""

#: distributions-mixture_same_family.R:20
msgid "Mixture distribution must be distr_categorical."
msgstr ""

#: distributions-mixture_same_family.R:24
msgid "Component distribution must be an instance of torch_Distribution."
msgstr ""

#: distributions-mixture_same_family.R:32
msgid ""
"Mixture distribution component ({km}) does not equal "
"component_distribution$batch_shape[-1] ({kc})."
msgstr ""

#: distributions-multivariate_normal.R:81
msgid "loc must be at least one-dimensional."
msgstr ""

#: distributions-multivariate_normal.R:85
msgid ""
"Exactly one of covariance_matrix or precision_matrix or scale_tril may be "
"specified."
msgstr ""

#: distributions-multivariate_normal.R:92
msgid "scale_tril matrix must be at least two-dimensional"
msgstr ""

#: distributions-multivariate_normal.R:93 distributions-multivariate_normal.R:103 distributions-multivariate_normal.R:115
msgid "with optional leading batch dimensions"
msgstr ""

#: distributions-multivariate_normal.R:102
msgid "covariance_matrix matrix must be at least two-dimensional"
msgstr ""

#: distributions-multivariate_normal.R:114
msgid "precision_matrix matrix must be at least two-dimensional"
msgstr ""

#: distributions-utils.R:24
msgid "Input arguments must all be instances of numeric,"
msgstr ""

#: distributions-utils.R:25
msgid "torch_tensor or objects implementing __torch_function__."
msgstr ""

#: distributions.R:169
msgid "The value argument to log_prob must be a Tensor"
msgstr ""

#: distributions.R:176
msgid ""
"The right-most size of value must match event_shape:\n"
"           {value$size()} vs {private$.event_shape}."
msgstr ""

#: distributions.R:192
msgid ""
"Value is not broadcastable with\n"
"             batch_shape+event_shape: {actual_shape} vs {expected_shape}."
msgstr ""

#: distributions.R:199
msgid "The value argument must be within the support"
msgstr ""

#: distributions.R:275
msgid "Subclass {paste0(class(self), collapse = ' ')} of"
msgstr ""

#: distributions.R:276
msgid "{paste0(class(cls), collapse = ' ')}"
msgstr ""

#: distributions.R:277
msgid "that defines a custom `initialize()` method"
msgstr ""

#: distributions.R:278
msgid "must also define a custom `expand()` method."
msgstr ""

#: dtype.R:162
msgid "One of the objects is not a dtype. Comparison is not possible."
msgstr ""

#: generator.R:17
msgid "bit64 is required to correctly show the seed."
msgstr ""

#: generator.R:24
msgid "Seed must an integer or integer64."
msgstr ""

#: install.R:542
msgid "Unexpected value"
msgstr ""

#: ivalue.R:12
msgid "Argument 'x' must be a list."
msgstr ""

#: ivalue.R:29
msgid "Argument 'x' must be scalar atomic."
msgstr ""

#: lantern_load.R:6
msgid "Torch is not installed, please run 'install_torch()'."
msgstr ""

#: linalg.R:269
msgid "`tol` argument is deprecated in favor of `atol` and `rtol`."
msgstr ""

#: linalg.R:954
msgid "`rcond` is deprecated in favor of `rtol`."
msgstr ""

#: nn-activation.R:760
msgid "embed_dim must be divisible by num_heads"
msgstr ""

#: nn-batchnorm.R:42 nn-init.R:229 nn-init.R:421
msgid "not implemented"
msgstr ""

#: nn-batchnorm.R:177
msgid "expected 2D or 3D input (got {input$dim()}D input)"
msgstr ""

#: nn-batchnorm.R:250
msgid "expected 4D input (got {input$dim()}D input)"
msgstr ""

#: nn-batchnorm.R:329
msgid "expected 5D input (got {input$dim()}D input)"
msgstr ""

#: nn-conv.R:10
msgid "`in_channels` must be divisible by groups"
msgstr ""

#: nn-conv.R:14
msgid "`out_channels` must be divisible by groups"
msgstr ""

#: nn-conv.R:21
msgid ""
"padding_mode must be one of [{paste(valid_padding_modes, collapse = ', ')}],"
msgstr ""

#: nn-conv.R:22
msgid "but got padding_mode='{padding_mode}'."
msgstr ""

#: nn-conv.R:509
msgid "Only `zeros` padding is supported."
msgstr ""

#: nn-conv.R:529
msgid "output_size must have {k} or {k+2} elements (got {length(output_size)})"
msgstr ""

#: nn-conv.R:550
msgid "requested an output of size {output_size}, but valid"
msgstr ""

#: nn-conv.R:551
msgid "sizes range from {min_size} to {max_size} (for an input"
msgstr ""

#: nn-conv.R:552
msgid "of size {input$size()[-c(1,2)]}"
msgstr ""

#: nn-conv.R:686
msgid "Only `zeros` padding mode is supported for ConvTranspose1d"
msgstr ""

#: nn-conv.R:843
msgid "Only `zeros` padding mode is supported for ConvTranspose2d"
msgstr ""

#: nn-conv.R:997
msgid "Only `zeros` padding mode is supported for ConvTranspose3d"
msgstr ""

#: nn-dropout.R:8
msgid "dropout probability has to be between 0 and 1 but got {p}"
msgstr ""

#: nn-init.R:18
msgid "mean is more than 2 std from [a, b] in nn.init.trunc_normal_."
msgstr ""

#: nn-init.R:19
msgid "The distribution of values may be incorrect."
msgstr ""

#: nn-init.R:89
msgid "Unsupported nonlinearity: {nonlinearity}"
msgstr ""

#: nn-loss.R:998
msgid "only p == 1 or p == 2 are supported."
msgstr ""

#: nn-loss.R:1001
msgid "weight must be NULL or 1-dimensional"
msgstr ""

#: nn-pooling.R:685 nn-pooling.R:758
msgid "both output_size and output_ratio are NULL"
msgstr ""

#: nn-pooling.R:689 nn-pooling.R:762
msgid "both output_size and output_ratio are not NULL"
msgstr ""

#: nn-pooling.R:694 nn-pooling.R:767
msgid "output_ratio must be between 0 and 1."
msgstr ""

#: nn-rnn.R:37
msgid "dropout option adds dropout after all but last"
msgstr ""

#: nn-rnn.R:38
msgid "recurrent layer, so non-zero dropout expects"
msgstr ""

#: nn-rnn.R:39
msgid "num_layers greater than 1, but got dropout={dropout} and"
msgstr ""

#: nn-rnn.R:40
msgid "num_layers={num_layers}"
msgstr ""

#: nn-rnn.R:53
msgid "Unrecognized RNN mode: {mode}"
msgstr ""

#: nn-rnn.R:273
msgid "No cudnn backend for mode '{mode}'"
msgstr ""

#: nn-rnn.R:395
msgid "Unknown nonlinearity '{self$nonlinearity}'"
msgstr ""

#: nn-sparse.R:67 nn-sparse.R:70 nn-sparse.R:167 nn-sparse.R:170
msgid "`padding_idx` must be within num_embeddings"
msgstr ""

#: nn-sparse.R:87 nn-sparse.R:187
msgid "Shape of `.weight` does not match num_embeddings and embedding_dim"
msgstr ""

#: nn-utils.R:7
msgid "Input dimension should be at least {length(out_size) + 1}."
msgstr ""

#: nn.R:22
msgid "Forward method is not implemented"
msgstr ""

#: nn.R:106
msgid ""
"nn.Module.to only accepts floating point '\n"
"                      'dtypes, but got desired dtype {dtype}"
msgstr ""

#: nn.R:192
msgid "Could not find {key} in the state_dict."
msgstr ""

#: nn.R:278
msgid "It's not possible to modify the parameters list."
msgstr ""

#: nn.R:279 nn.R:295
msgid "You can modify the parameter in-place or use"
msgstr ""

#: nn.R:280 nn.R:296
msgid "`module$parameter_name <- new_value`"
msgstr ""

#: nn.R:294
msgid "It's not possible to modify the buffers list."
msgstr ""

#: nn.R:310
msgid "It's not possible to modify the modules list."
msgstr ""

#: nn.R:311 nn.R:331
msgid "You can modify the modules in-place"
msgstr ""

#: nn.R:330
msgid "It's not possible to modify the children list."
msgstr ""

#: nn.R:351
msgid "`x` must be a tensor."
msgstr ""

#: nn_adaptive.R:108
msgid "not yet implemented"
msgstr ""

#: nnf-activation.R:81
msgid "not yet implemented."
msgstr ""

#: nnf-activation.R:798
msgid "Input should be a tensor and got '{class(input)}."
msgstr ""

#: nnf-dropout.R:42
msgid ""
"dropout2d: Received a {inp_dim}-D input to dropout2d, which is deprecated"
msgstr ""

#: nnf-dropout.R:43
msgid "and will result in an error in a future release. To retain the behavior"
msgstr ""

#: nnf-dropout.R:44
msgid ""
"and silence this warning, please use dropout instead. Note that dropout2d"
msgstr ""

#: nnf-dropout.R:45
msgid ""
"exists to provide channel-wise dropout on inputs with 2 spatial dimensions,"
msgstr ""

#: nnf-dropout.R:46
msgid ""
"a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs)."
msgstr ""

#: nnf-dropout.R:52
msgid ""
"dropout2d: Received a 3D input to dropout2d and assuming that channel-wise"
msgstr ""

#: nnf-dropout.R:53
msgid ""
"1D dropout behavior is desired - input is interpreted as shape (N, C, L), "
"where C"
msgstr ""

#: nnf-dropout.R:54
msgid ""
"is the channel dim. This behavior will change in a future release to "
"interpret the"
msgstr ""

#: nnf-dropout.R:55
msgid ""
"input as one without a batch dimension, i.e. shape (C, H, W). To maintain "
"the 1D"
msgstr ""

#: nnf-dropout.R:56
msgid ""
"channel-wise dropout behavior, please switch to using dropout1d instead."
msgstr ""

#: nnf-embedding.R:83
msgid "if input is 2D, then offsets has to be NULL"
msgstr ""

#: nnf-loss.R:28
msgid ""
"reduction: 'mean' divides the total loss by both the batch size and the "
"support size."
msgstr ""

#: nnf-loss.R:29
msgid ""
"'batchmean' divides only by the batch size, and aligns with the KL div math "
"definition."
msgstr ""

#: nnf-loss.R:30
msgid ""
"'mean' will be changed to behave the same as 'batchmean' in the next major "
"release."
msgstr ""

#: nnf-loss.R:63
msgid ""
"Using a target size {target_shape} that is different to the input size "
"{input_shape}."
msgstr ""

#: nnf-loss.R:64
msgid "This will likely lead to incorrect results due to broadcasting."
msgstr ""

#: nnf-loss.R:65
msgid "Please ensure they have the same size."
msgstr ""

#: nnf-loss.R:216
msgid "reduction is not valid."
msgstr ""

#: nnf-pooling.R:278
msgid "output_size should be a sequence containing"
msgstr ""

#: nnf-pooling.R:279
msgid "{length(kernel_size)} or {length(kernel_size) + 2} elements"
msgstr ""

#: nnf-pooling.R:280
msgid "but it has a length of '{length(output_size)}'"
msgstr ""

#: nnf-pooling.R:452
msgid "output_ratio should not be NULL if output_size is NULL"
msgstr ""

#: nnf-upsampling.R:60
msgid "only one of size or scale_factor should be defined"
msgstr ""

#: nnf-upsampling.R:67
msgid ""
"size shape must match input shape. Input is {dim}D, size is {length(size)}"
msgstr ""

#: nnf-upsampling.R:77
msgid ""
"scale_factor shape must match input shape. Input is {dim}D, size is "
"{length(size)}"
msgstr ""

#: nnf-upsampling.R:82
msgid "either size or scale_factor should be defined"
msgstr ""

#: nnf-upsampling.R:144
msgid "Got 3D input, but bilinear mode needs 4D input"
msgstr ""

#: nnf-upsampling.R:148
msgid "Got 3D input, but trilinear mode needs 5D input"
msgstr ""

#: nnf-upsampling.R:152
msgid "Got 4D input, but trilinear mode needs 3D input"
msgstr ""

#: nnf-upsampling.R:164
msgid "Got 4D input, but trilinear mode needs 5D input"
msgstr ""

#: nnf-upsampling.R:168
msgid "Got 5D input, but trilinear mode needs 3D input"
msgstr ""

#: nnf-upsampling.R:172
msgid "Got 5D input, but bilinear mode needs 4D input"
msgstr ""

#: nnf-upsampling.R:192
msgid "Input Error: Only 3D, 4D and 5D input Tensors supported"
msgstr ""

#: nnf-upsampling.R:193
msgid ""
"(got {input$dim()}D) for the modes: nearest | linear | bilinear | bicubic | "
"trilinear"
msgstr ""

#: nnf-upsampling.R:194
msgid "(got {mode})"
msgstr ""

#: nnf-vision.R:97
msgid "Unknown mode name '{mode}'. Supported modes are 'bilinear'"
msgstr ""

#: nnf-vision.R:98
msgid "and 'nearest'."
msgstr ""

#: nnf-vision.R:111
msgid "Unknown padding mode name '{padding_mode}'. Supported modes are"
msgstr ""

#: nnf-vision.R:112
msgid "'zeros', 'border' and 'reflection'."
msgstr ""

#: operators.R:326 operators.R:334 operators.R:342 operators.R:358 operators.R:374
msgid "Torch tensors do not have NAs!"
msgstr ""

#: optim-adadelta.R:51 optim-adagrad.R:38 optim-adam.R:36 optim-asgd.R:32 optim-rmsprop.R:40 optim-rprop.R:30 optim-sgd.R:61
msgid "Invalid learning rate: {lr}"
msgstr ""

#: optim-adadelta.R:55
msgid "Invalid rho value: {rho}"
msgstr ""

#: optim-adadelta.R:59 optim-adagrad.R:54 optim-rmsprop.R:43
msgid "Invalid epsilon value: {eps}"
msgstr ""

#: optim-adadelta.R:63 optim-adagrad.R:46 optim-asgd.R:36 optim-rmsprop.R:49 optim-sgd.R:69
msgid "Invalid weight_decay value: {weight_decay}"
msgstr ""

#: optim-adagrad.R:42
msgid "Invalid lr_decay value: {lr_decay}"
msgstr ""

#: optim-adagrad.R:50
msgid "Invalid initial_accumulator_value value: {initial_accumulator_value}"
msgstr ""

#: optim-adam.R:40
msgid "Invalid eps: {eps}"
msgstr ""

#: optim-adam.R:44
msgid "Invalid beta parameter at index 1"
msgstr ""

#: optim-adam.R:48
msgid "Invalid beta parameter at index 2"
msgstr ""

#: optim-adam.R:52
msgid "Invalid weight decay value: {weight_decay}"
msgstr ""

#: optim-lbfgs.R:371
msgid "LBFGS doesn't support per-parameter options"
msgstr ""

#: optim-lbfgs.R:372
msgid "(parameter groups)"
msgstr ""

#: optim-lbfgs.R:527
msgid "only strong_wolfe is supported"
msgstr ""

#: optim-lr_scheduler.R:18
msgid "not an optimizer"
msgstr ""

#: optim-lr_scheduler.R:35
msgid "param 'inital_lr' not is not specified."
msgstr ""

#: optim-lr_scheduler.R:179 optim-lr_scheduler.R:237
msgid "lr_lambda length ({i}) is different from the number of"
msgstr ""

#: optim-lr_scheduler.R:180 optim-lr_scheduler.R:238
msgid "optimizer$param_grpups ({j})"
msgstr ""

#: optim-lr_scheduler.R:425
msgid "You must define either total_steps OR (epochs AND steps_per_epoch)"
msgstr ""

#: optim-lr_scheduler.R:428
msgid "Expected positive integer total_steps, but got {total_steps}"
msgstr ""

#: optim-lr_scheduler.R:434
msgid "Expected positive integer epochs, but got {epochs}"
msgstr ""

#: optim-lr_scheduler.R:438
msgid "Expected positive integer steps_per_epoch, but got {steps_per_epoch}"
msgstr ""

#: optim-lr_scheduler.R:449
msgid "Expected float between 0 and 1 pct_start, but got {pct_start}"
msgstr ""

#: optim-lr_scheduler.R:454
msgid ""
"anneal_strategy must by one of 'cos' or 'linear', instead got "
"{anneal_strategy}"
msgstr ""

#: optim-lr_scheduler.R:478
msgid "optimizer must support momentum with `cycle momentum` enabled"
msgstr ""

#: optim-lr_scheduler.R:506
msgid "expected {length(optimizer$param_groups)} values for {name}"
msgstr ""

#: optim-lr_scheduler.R:507
msgid "but got {length(param)}"
msgstr ""

#: optim-lr_scheduler.R:529
msgid ""
"Tried to step {step_num+1} times. The specified number of total steps is "
"{self$total_steps}"
msgstr ""

#: optim-lr_scheduler.R:632
msgid "Factor should be < 1.0"
msgstr ""

#: optim-lr_scheduler.R:639
msgid "expected {length(optimizer$param_groups} min_lrs, got {length(min_lr)}"
msgstr ""

#: optim-lr_scheduler.R:721
msgid "mode {mode} is unknown!"
msgstr ""

#: optim-lr_scheduler.R:724
msgid "threshold mode {threshold_mode} is unknown!"
msgstr ""

#: optim-rmsprop.R:46 optim-sgd.R:65
msgid "Invalid momentum value: {momentum}"
msgstr ""

#: optim-rmsprop.R:52
msgid "Invalid alpha value: {alpha}"
msgstr ""

#: optim-rprop.R:33
msgid "Invalid eta values: {etas[[1]]}, {etas[[2]]}"
msgstr ""

#: optim-sgd.R:73
msgid "Nesterov momentum requires a momentum and zero dampening"
msgstr ""

#: optim.R:37
msgid "Wrong parameters specification."
msgstr ""

#: optim.R:48
msgid "`param_group` is not named"
msgstr ""

#: optim.R:59
msgid "optimizer can only optimize Tensors,"
msgstr ""

#: optim.R:60
msgid "but one of the params is {class(param)}"
msgstr ""

#: optim.R:65
msgid "can't optimize a non-leaf Tensor"
msgstr ""

#: optim.R:73
msgid "parameter group didn't specify a value of required"
msgstr ""

#: optim.R:74
msgid "optimization parameter {nm}"
msgstr ""

#: optim.R:113
msgid "Loaded state dict has a different number of parameter groups"
msgstr ""

#: optim.R:118
msgid ""
"Loaded state dict has contains a parameter group that doesn't match the size "
"of optimizers group."
msgstr ""

#: package.R:103
msgid "Aborted."
msgstr ""

#: save.R:304
msgid "currently unsuported"
msgstr ""

#: script_module.R:18
msgid "ScriptModule does not support non persistent buffers."
msgstr ""

#: script_module.R:29
msgid "Script modules can only register Script modules children."
msgstr ""

#: script_module.R:173
msgid ""
"Forward is not defined. Methods from submodules of traced modules are not "
"traced. Are you trying to call from a submodule?"
msgstr ""

#: tensor.R:58
msgid "Indexing starts at 1 and got a 0."
msgstr ""

#: tensor.R:118
msgid "You must pass a cuda device."
msgstr ""

#: tensor.R:183 tensor.R:190 wrapers.R:234
msgid "start indexing starts at 1"
msgstr ""

#: tensor.R:201 tensor.R:219
msgid "Can't set other and dim arguments."
msgstr ""

#: tensor.R:336
msgid "The tensor doesn't have names so you can't rename a dimension."
msgstr ""

#: tensor.R:416
msgid "Can't convert cuda tensor to R. Convert to cpu tensor before."
msgstr ""

#: tensor.R:427
msgid ""
"Converting integers > .Machine$integer.max is undefined and returns wrong "
"results. Use as.integer64(x)"
msgstr ""

#: trace.R:70
msgid "You must initialize the nn_module before tracing."
msgstr ""

#: trace.R:82
msgid "jit_trace needs a function or nn_module."
msgstr ""

#: trace.R:126 trace.R:343
msgid "Only `script_function` or `script_module` can be saved with `jit_save`."
msgstr ""

#: trace.R:281
msgid "`mod` must be a `nn_module()`."
msgstr ""

#: trace.R:285
msgid "Arguments passed trough `...` must be named."
msgstr ""

#: trace.R:292
msgid "Method '{name}' does not exist in `mod` and therefore can't be traced."
msgstr ""

#: type-info.R:30
msgid "dtype must be an integer type."
msgstr ""

#: type-info.R:69
msgid "dtype must be a float type."
msgstr ""

#: utils-data-collate.R:31
msgid "Can't collate data of class: '{class(data)}'"
msgstr ""

#: utils-data-collate.R:46
msgid "Can't convert data of class: '{class(data)}'"
msgstr ""

#: utils-data-dataloader.R:121
msgid "Could not find an object with name '{names(worker_globals)[b]}'."
msgstr ""

#: utils-data-dataloader.R:441
msgid "Failed starting the worker."
msgstr ""

#: utils-data-dataloader.R:488 utils-data-dataloader.R:531
msgid "dataloader worker timed out."
msgstr ""

#: utils-data-dataloader.R:644
msgid ""
"Datasets used with parallel dataloader (num_workers > 0) shouldn't have "
"fields containing tensors as they can't be correctly passed to the wroker "
"subprocesses."
msgstr ""

#: utils-data-dataloader.R:645
msgid "A field named '{nm}' exists."
msgstr ""

#: utils-data-enum.R:41
msgid ""
"The `enumerate` construct is deprecated in favor of the `coro::loop` syntax."
msgstr ""

#: utils-data-enum.R:42
msgid "See https://github.com/mlverse/torch/issues/558 for more information."
msgstr ""

#: utils-data.R:226
msgid "all tensors must have the same size in the first dimension."
msgstr ""

#: wrapers.R:25
msgid "argument 'out' must be a list of Tensors."
msgstr ""

#: wrapers.R:30
msgid "expected tuple of"
msgstr ""

#: wrapers.R:30
msgid "elements but got"
msgstr ""

#: wrapers.R:106
msgid "argument 'window_length' must be int, not NULL"
msgstr ""

#: wrapers.R:184
msgid "tensordot expects dims >= 1, but got {dims}"
msgstr ""

#: wrapers.R:499
msgid "size is set, but one of mean or std is not a scalar value."
msgstr ""

#: wrapers.R:505
msgid "options is set, but one of mean or std is not a scalar value."
msgstr ""

#: wrapers.R:514
msgid "size is not set."
msgstr ""

#: wrapers.R:553
msgid "Please report a bug report in GitHub"
msgstr ""
