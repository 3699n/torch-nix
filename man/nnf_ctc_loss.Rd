% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/gen-nn-functional-docs.R,
%   R/gen-nn-functional-examples.R
\name{nnf_ctc_loss}
\alias{nnf_ctc_loss}
\title{Ctc_loss}
\arguments{
\item{log_probs}{NA \eqn{(T, N, C)} where \verb{C = number of characters in alphabet including blank},        \verb{T = input length}, and \verb{N = batch size}.        The logarithmized probabilities of the outputs        (e.g. obtained with \code{\link{torch_nn.functional.log_softmax}}).}

\item{targets}{NA \eqn{(N, S)} or \code{(sum(target_lengths))}.        Targets cannot be blank. In the second form, the targets are assumed to be concatenated.}

\item{input_lengths}{NA \eqn{(N)}.        Lengths of the inputs (must each be \eqn{\leq T})}

\item{target_lengths}{NA \eqn{(N)}.        Lengths of the targets}

\item{blank}{(int, optional) Blank label. Default \eqn{0}.}

\item{reduction}{(string, optional) Specifies the reduction to apply to the output:        \code{'none'} | \code{'mean'} | \code{'sum'}. \code{'none'}: no reduction will be applied,        \code{'mean'}: the output losses will be divided by the target lengths and        then the mean over the batch is taken, \code{'sum'}: the output will be        summed. Default: \code{'mean'}}

\item{zero_infinity}{(bool, optional) Whether to zero infinite losses and the associated gradients.        Default: \code{False}        Infinite losses mainly occur when the inputs are too short        to be aligned to the targets.}
}
\description{
The Connectionist Temporal Classification loss.
}
\details{
\preformatted{See `~torch.nn.CTCLoss` for details.

.. include:: cudnn_deterministic.rst
.. include:: cuda_deterministic_backward.rst
}
}
\examples{

log_probs = torch_randn(c(50, 16, 20))$log_softmax(2)$detach()$requires_grad_()
targets = torch_randint(1, 20, list(16, 30), dtype=torch_long())
input_lengths = torch_full(list(16,), 50, dtype=torch_long())
target_lengths = torch_randint(10,30,list(16,), dtype=torch_long())
loss = F$ctc_loss(log_probs, targets, input_lengths, target_lengths)
loss$backward()
}
