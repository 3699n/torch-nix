% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/gen-nn-functional-docs.R,
%   R/gen-nn-functional-examples.R
\name{nnf_cross_entropy}
\alias{nnf_cross_entropy}
\title{Cross_entropy}
\arguments{
\item{input}{(Tensor) \eqn{(N, C)} where \verb{C = number of classes} or \eqn{(N, C, H, W)}        in case of 2D Loss, or \eqn{(N, C, d_1, d_2, ..., d_K)} where \eqn{K \geq 1}        in the case of K-dimensional loss.}

\item{target}{(Tensor) \eqn{(N)} where each value is \eqn{0 \leq \text{targets}[i] \leq C-1},        or \eqn{(N, d_1, d_2, ..., d_K)} where \eqn{K \geq 1} for        K-dimensional loss.}

\item{weight}{(Tensor, optional) a manual rescaling weight given to each        class. If given, has to be a Tensor of size \code{C}}

\item{size_average}{(bool, optional) Deprecated (see \code{reduction}). By default,        the losses are averaged over each loss element in the batch. Note that for        some losses, there multiple elements per sample. If the field \code{size_average}        is set to \code{False}, the losses are instead summed for each minibatch. Ignored        when reduce is \code{False}. Default: \code{True}}

\item{ignore_index}{(int, optional) Specifies a target value that is ignored        and does not contribute to the input gradient. When \code{size_average} is        \code{True}, the loss is averaged over non-ignored targets. Default: -100}

\item{reduce}{(bool, optional) Deprecated (see \code{reduction}). By default, the        losses are averaged or summed over observations for each minibatch depending        on \code{size_average}. When \code{reduce} is \code{False}, returns a loss per        batch element instead and ignores \code{size_average}. Default: \code{True}}

\item{reduction}{(string, optional) Specifies the reduction to apply to the output:        \code{'none'} | \code{'mean'} | \code{'sum'}. \code{'none'}: no reduction will be applied,        \code{'mean'}: the sum of the output will be divided by the number of        elements in the output, \code{'sum'}: the output will be summed. Note: \code{size_average}        and \code{reduce} are in the process of being deprecated, and in the meantime,        specifying either of those two args will override \code{reduction}. Default: \code{'mean'}}
}
\description{
This criterion combines \code{log_softmax} and \code{nll_loss} in a single
function.
}
\details{
\preformatted{See `~torch.nn.CrossEntropyLoss` for details.
}
}
\examples{

input = torch_randn(3, 5, requires_grad=TRUE)
target = torch_randint(5, list(3,), dtype=torch_int64())
loss = F$cross_entropy(input, target)
loss$backward()
}
