#' Adaptive_avg_pool1d
#'
#' @section adaptive_avg_pool1d(input, output_size) -> Tensor :
#'
#' Applies a 1D adaptive average pooling over an input signal composed of
#' several input planes.
#' 
#' See `~torch.nn.AdaptiveAvgPool1d` for details and output shape.
#'
#'
#' @param output_size NA the target output size (single integer)
#'
#' @name nnf_adaptive_avg_pool1d
#'
#' @export
NULL


#' Adaptive_avg_pool2d
#'
#' Applies a 2D adaptive average pooling over an input signal composed of
#'     several input planes.
#' 
#'     See `~torch.nn.AdaptiveAvgPool2d` for details and output shape.
#'
#' @param output_size NA the target output size (single integer or        double-integer tuple)
#'
#' @name nnf_adaptive_avg_pool2d
#'
#' @export
NULL


#' Adaptive_avg_pool3d
#'
#' Applies a 3D adaptive average pooling over an input signal composed of
#'     several input planes.
#' 
#'     See `~torch.nn.AdaptiveAvgPool3d` for details and output shape.
#'
#' @param output_size NA the target output size (single integer or        triple-integer tuple)
#'
#' @name nnf_adaptive_avg_pool3d
#'
#' @export
NULL


#' Adaptive_max_pool1d
#'
#' Applies a 1D adaptive max pooling over an input signal composed of
#'     several input planes.
#' 
#'     See `~torch.nn.AdaptiveMaxPool1d` for details and output shape.
#'
#' @param output_size NA the target output size (single integer)
#' @param return_indices NA whether to return pooling indices. Default: ``False``
#'
#' @name nnf_adaptive_max_pool1d
#'
#' @export
NULL


#' Adaptive_max_pool1d_with_indices
#'
#' Applies a 1D adaptive max pooling over an input signal composed of
#'     several input planes.
#' 
#'     See `~torch.nn.AdaptiveMaxPool1d` for details and output shape.
#'
#' @param output_size NA the target output size (single integer)
#' @param return_indices NA whether to return pooling indices. Default: ``False``
#'
#' @name nnf_adaptive_max_pool1d_with_indices
#'
#' @export
NULL


#' Adaptive_max_pool2d
#'
#' Applies a 2D adaptive max pooling over an input signal composed of
#'     several input planes.
#' 
#'     See `~torch.nn.AdaptiveMaxPool2d` for details and output shape.
#'
#' @param output_size NA the target output size (single integer or        double-integer tuple)
#' @param return_indices NA whether to return pooling indices. Default: ``False``
#'
#' @name nnf_adaptive_max_pool2d
#'
#' @export
NULL


#' Adaptive_max_pool2d_with_indices
#'
#' Applies a 2D adaptive max pooling over an input signal composed of
#'     several input planes.
#' 
#'     See `~torch.nn.AdaptiveMaxPool2d` for details and output shape.
#'
#' @param output_size NA the target output size (single integer or        double-integer tuple)
#' @param return_indices NA whether to return pooling indices. Default: ``False``
#'
#' @name nnf_adaptive_max_pool2d_with_indices
#'
#' @export
NULL


#' Adaptive_max_pool3d
#'
#' Applies a 3D adaptive max pooling over an input signal composed of
#'     several input planes.
#' 
#'     See `~torch.nn.AdaptiveMaxPool3d` for details and output shape.
#'
#' @param output_size NA the target output size (single integer or        triple-integer tuple)
#' @param return_indices NA whether to return pooling indices. Default: ``False``
#'
#' @name nnf_adaptive_max_pool3d
#'
#' @export
NULL


#' Adaptive_max_pool3d_with_indices
#'
#' Applies a 3D adaptive max pooling over an input signal composed of
#'     several input planes.
#' 
#'     See `~torch.nn.AdaptiveMaxPool3d` for details and output shape.
#'
#' @param output_size NA the target output size (single integer or        triple-integer tuple)
#' @param return_indices NA whether to return pooling indices. Default: ``False``
#'
#' @name nnf_adaptive_max_pool3d_with_indices
#'
#' @export
NULL


#' Affine_grid
#'
#' @section Generates a 2D or 3D flow field (sampling grid), given a batch of :
#'
#' Generates a 2D or 3D flow field (sampling grid), given a batch of
#'     affine matrices `theta`.
#' 
#'     .. note::
#'         This function is often used in conjunction with [`grid_sample`]
#'         to build `Spatial Transformer Networks`_ .
#'
#'
#' @param theta (Tensor) input batch of affine matrices with shape        (\eqn{N \times 2 \times 3}) for 2D or        (\eqn{N \times 3 \times 4}) for 3D
#' @param size (torch.Size) the target output image size.        (\eqn{N \times C \times H \times W} for 2D or        \eqn{N \times C \times D \times H \times W} for 3D)        Example: torch.Size((32, 3, 24, 24))
#' @param align_corners (bool, optional) if ``True``, consider ``-1`` and ``1``        to refer to the centers of the corner pixels rather than the image corners.        Refer to [`grid_sample`] for a more complete description.        A grid generated by [`affine_grid`] should be passed to [`grid_sample`]        with the same setting for this option.        Default: ``False``
#'
#' @name nnf_affine_grid
#'
#' @export
NULL


#' Alpha_dropout
#'
#' Applies alpha dropout to the input.
#' 
#'     See `~torch.nn.AlphaDropout` for details.
#'
#'
#'
#' @name nnf_alpha_dropout
#'
#' @export
NULL


#' Avg_pool1d
#'
#' @section avg_pool1d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True) -> Tensor :
#'
#' Applies a 1D average pooling over an input signal composed of several
#' input planes.
#' 
#' See `~torch.nn.AvgPool1d` for details and output shape.
#'
#'
#' @param input NA input tensor of shape \eqn{(\text{minibatch} , \text{in\_channels} , iW)}
#' @param kernel_size NA the size of the window. Can be a single number or a      tuple `(kW,)`
#' @param stride NA the stride of the window. Can be a single number or a tuple      `(sW,)`. Default: `kernel_size`
#' @param padding NA implicit zero paddings on both sides of the input. Can be a      single number or a tuple `(padW,)`. Default: 0
#' @param ceil_mode NA when True, will use `ceil` instead of `floor` to compute the        output shape. Default: ``False``
#' @param count_include_pad NA when True, will include the zero-padding in the        averaging calculation. Default: ``True``
#'
#' @name nnf_avg_pool1d
#'
#' @export
NULL


#' Avg_pool2d
#'
#' @section avg_pool2d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True, divisor_override=None) -> Tensor :
#'
#' Applies 2D average-pooling operation in \eqn{kH \times kW} regions by step size
#' \eqn{sH \times sW} steps. The number of output features is equal to the number of
#' input planes.
#' 
#' See `~torch.nn.AvgPool2d` for details and output shape.
#'
#'
#' @param input NA input tensor \eqn{(\text{minibatch} , \text{in\_channels} , iH , iW)}
#' @param kernel_size NA size of the pooling region. Can be a single number or a      tuple `(kH, kW)`
#' @param stride NA stride of the pooling operation. Can be a single number or a      tuple `(sH, sW)`. Default: `kernel_size`
#' @param padding NA implicit zero paddings on both sides of the input. Can be a      single number or a tuple `(padH, padW)`. Default: 0
#' @param ceil_mode NA when True, will use `ceil` instead of `floor` in the formula        to compute the output shape. Default: ``False``
#' @param count_include_pad NA when True, will include the zero-padding in the        averaging calculation. Default: ``True``
#' @param divisor_override NA if specified, it will be used as divisor, otherwise         size of the pooling region will be used. Default: None
#'
#' @name nnf_avg_pool2d
#'
#' @export
NULL


#' Avg_pool3d
#'
#' @section avg_pool3d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True, divisor_override=None) -> Tensor :
#'
#' Applies 3D average-pooling operation in \eqn{kT \times kH \times kW} regions by step
#' size \eqn{sT \times sH \times sW} steps. The number of output features is equal to
#' \eqn{\lfloor\frac{\text{input planes}}{sT}\rfloor}.
#' 
#' See `~torch.nn.AvgPool3d` for details and output shape.
#'
#'
#' @param input NA input tensor \eqn{(\text{minibatch} , \text{in\_channels} , iT \times iH , iW)}
#' @param kernel_size NA size of the pooling region. Can be a single number or a      tuple `(kT, kH, kW)`
#' @param stride NA stride of the pooling operation. Can be a single number or a      tuple `(sT, sH, sW)`. Default: `kernel_size`
#' @param padding NA implicit zero paddings on both sides of the input. Can be a      single number or a tuple `(padT, padH, padW)`, Default: 0
#' @param ceil_mode NA when True, will use `ceil` instead of `floor` in the formula        to compute the output shape
#' @param count_include_pad NA when True, will include the zero-padding in the        averaging calculation
#' @param divisor_override NA if specified, it will be used as divisor, otherwise        size of the pooling region will be used. Default: None
#'
#' @name nnf_avg_pool3d
#'
#' @export
NULL





#' Bilinear
#'
#' Applies a bilinear transformation to the incoming data:
#'     \eqn{y = x_1 A x_2 + b}
#' 
#'     Shape:
#' 
#'         - input1: \eqn{(N, *, H_{in1})} where \eqn{H_{in1}=\text{in1\_features}}
#'           and \eqn{*} means any number of additional dimensions.
#'           All but the last dimension of the inputs should be the same.
#'         - input2: \eqn{(N, *, H_{in2})} where \eqn{H_{in2}=\text{in2\_features}}
#'         - weight: \eqn{(\text{out\_features}, \text{in1\_features},
#'           \text{in2\_features})}
#'         - bias: \eqn{(\text{out\_features})}
#'         - output: \eqn{(N, *, H_{out})} where \eqn{H_{out}=\text{out\_features}}
#'           and all but the last dimension are the same shape as the input.
#'
#'
#'
#' @name nnf_bilinear
#'
#' @export
NULL


#' Binary_cross_entropy
#'
#' Function that measures the Binary Cross Entropy
#'     between the target and the output.
#' 
#'     See `~torch.nn.BCELoss` for details.
#'
#' @param input NA Tensor of arbitrary shape
#' @param target NA Tensor of the same shape as input
#' @param weight (Tensor, optional) a manual rescaling weight            if provided it's repeated to match input tensor shape
#' @param size_average (bool, optional) Deprecated (see `reduction`). By default,        the losses are averaged over each loss element in the batch. Note that for        some losses, there multiple elements per sample. If the field `size_average`        is set to ``False``, the losses are instead summed for each minibatch. Ignored        when reduce is ``False``. Default: ``True``
#' @param reduce (bool, optional) Deprecated (see `reduction`). By default, the        losses are averaged or summed over observations for each minibatch depending        on `size_average`. When `reduce` is ``False``, returns a loss per        batch element instead and ignores `size_average`. Default: ``True``
#' @param reduction (string, optional) Specifies the reduction to apply to the output:        ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,        ``'mean'``: the sum of the output will be divided by the number of        elements in the output, ``'sum'``: the output will be summed. Note: `size_average`        and `reduce` are in the process of being deprecated, and in the meantime,        specifying either of those two args will override `reduction`. Default: ``'mean'``
#'
#' @name nnf_binary_cross_entropy
#'
#' @export
NULL


#' Binary_cross_entropy_with_logits
#'
#' Function that measures Binary Cross Entropy between target and output
#'     logits.
#' 
#'     See `~torch.nn.BCEWithLogitsLoss` for details.
#'
#' @param input NA Tensor of arbitrary shape
#' @param target NA Tensor of the same shape as input
#' @param weight (Tensor, optional) a manual rescaling weight        if provided it's repeated to match input tensor shape
#' @param size_average (bool, optional) Deprecated (see `reduction`). By default,        the losses are averaged over each loss element in the batch. Note that for        some losses, there multiple elements per sample. If the field `size_average`        is set to ``False``, the losses are instead summed for each minibatch. Ignored        when reduce is ``False``. Default: ``True``
#' @param reduce (bool, optional) Deprecated (see `reduction`). By default, the        losses are averaged or summed over observations for each minibatch depending        on `size_average`. When `reduce` is ``False``, returns a loss per        batch element instead and ignores `size_average`. Default: ``True``
#' @param reduction (string, optional) Specifies the reduction to apply to the output:        ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,        ``'mean'``: the sum of the output will be divided by the number of        elements in the output, ``'sum'``: the output will be summed. Note: `size_average`        and `reduce` are in the process of being deprecated, and in the meantime,        specifying either of those two args will override `reduction`. Default: ``'mean'``
#' @param pos_weight (Tensor, optional) a weight of positive examples.            Must be a vector with length equal to the number of classes.
#'
#' @name nnf_binary_cross_entropy_with_logits
#'
#' @export
NULL


#' Boolean_dispatch
#'
#' Dispatches to either of 2 script functions based on a boolean argument.
#'     In TorchScript, the boolean argument must be constant so that the correct
#'     function to use can be determined at compile time.
#'
#'
#'
#' @name nnf_boolean_dispatch
#'
#' @export
NULL


#' Conv_tbc
#'
#' Applies a 1-dimensional sequence convolution over an input sequence.
#' Input and output dimensions are (Time, Batch, Channels) - hence TBC.
#'
#' @param input NA input tensor of shape \eqn{(\text{sequence length} \times batch \times \text{in\_channels})}
#' @param weight NA filter of shape (\eqn{\text{kernel width} \times \text{in\_channels} \times \text{out\_channels}})
#' @param bias NA bias of shape (\eqn{\text{out\_channels}})
#' @param pad NA number of timesteps to pad. Default: 0
#'
#' @name nnf_conv_tbc
#'
#' @export
NULL



#' Cosine_embedding_loss
#'
#' @section cosine_embedding_loss(input1, input2, target, margin=0, size_average=None, reduce=None, reduction='mean') -> Tensor :
#'
#' See `~torch.nn.CosineEmbeddingLoss` for details.
#'
#'
#'
#'
#' @name nnf_cosine_embedding_loss
#'
#' @export
NULL


#' Cosine_similarity
#'
#' @section cosine_similarity(x1, x2, dim=1, eps=1e-8) -> Tensor :
#'
#' Returns cosine similarity between x1 and x2, computed along dim.
#' 
#' \deqn{
#'     \text{similarity} = \dfrac{x_1 \cdot x_2}{\max(\Vert x_1 \Vert _2 \cdot \Vert x_2 \Vert _2, \epsilon)}
#' }
#'
#'
#' @param x1 (Tensor) First input.
#' @param x2 (Tensor) Second input (of size matching x1).
#' @param dim (int, optional) Dimension of vectors. Default: 1
#' @param eps (float, optional) Small value to avoid division by zero.        Default: 1e-8
#'
#' @name nnf_cosine_similarity
#'
#' @export
NULL


#' Cross_entropy
#'
#' This criterion combines `log_softmax` and `nll_loss` in a single
#'     function.
#' 
#'     See `~torch.nn.CrossEntropyLoss` for details.
#'
#' @param input (Tensor) \eqn{(N, C)} where `C = number of classes` or \eqn{(N, C, H, W)}        in case of 2D Loss, or \eqn{(N, C, d_1, d_2, ..., d_K)} where \eqn{K \geq 1}        in the case of K-dimensional loss.
#' @param target (Tensor) \eqn{(N)} where each value is \eqn{0 \leq \text{targets}[i] \leq C-1},        or \eqn{(N, d_1, d_2, ..., d_K)} where \eqn{K \geq 1} for        K-dimensional loss.
#' @param weight (Tensor, optional) a manual rescaling weight given to each        class. If given, has to be a Tensor of size `C`
#' @param size_average (bool, optional) Deprecated (see `reduction`). By default,        the losses are averaged over each loss element in the batch. Note that for        some losses, there multiple elements per sample. If the field `size_average`        is set to ``False``, the losses are instead summed for each minibatch. Ignored        when reduce is ``False``. Default: ``True``
#' @param ignore_index (int, optional) Specifies a target value that is ignored        and does not contribute to the input gradient. When `size_average` is        ``True``, the loss is averaged over non-ignored targets. Default: -100
#' @param reduce (bool, optional) Deprecated (see `reduction`). By default, the        losses are averaged or summed over observations for each minibatch depending        on `size_average`. When `reduce` is ``False``, returns a loss per        batch element instead and ignores `size_average`. Default: ``True``
#' @param reduction (string, optional) Specifies the reduction to apply to the output:        ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,        ``'mean'``: the sum of the output will be divided by the number of        elements in the output, ``'sum'``: the output will be summed. Note: `size_average`        and `reduce` are in the process of being deprecated, and in the meantime,        specifying either of those two args will override `reduction`. Default: ``'mean'``
#'
#' @name nnf_cross_entropy
#'
#' @export
NULL


#' Ctc_loss
#'
#' The Connectionist Temporal Classification loss.
#' 
#'     See `~torch.nn.CTCLoss` for details.
#' 
#'     .. include:: cudnn_deterministic.rst
#'     .. include:: cuda_deterministic_backward.rst
#'
#' @param log_probs NA \eqn{(T, N, C)} where `C = number of characters in alphabet including blank`,        `T = input length`, and `N = batch size`.        The logarithmized probabilities of the outputs        (e.g. obtained with [`torch_nn.functional.log_softmax`]).
#' @param targets NA \eqn{(N, S)} or `(sum(target_lengths))`.        Targets cannot be blank. In the second form, the targets are assumed to be concatenated.
#' @param input_lengths NA \eqn{(N)}.        Lengths of the inputs (must each be \eqn{\leq T})
#' @param target_lengths NA \eqn{(N)}.        Lengths of the targets
#' @param blank (int, optional) Blank label. Default \eqn{0}.
#' @param reduction (string, optional) Specifies the reduction to apply to the output:        ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,        ``'mean'``: the output losses will be divided by the target lengths and        then the mean over the batch is taken, ``'sum'``: the output will be        summed. Default: ``'mean'``
#' @param zero_infinity (bool, optional) Whether to zero infinite losses and the associated gradients.        Default: ``False``        Infinite losses mainly occur when the inputs are too short        to be aligned to the targets.
#'
#' @name nnf_ctc_loss
#'
#' @export
NULL


#' Dropout
#'
#' During training, randomly zeroes some of the elements of the input
#'     tensor with probability `p` using samples from a Bernoulli
#'     distribution.
#' 
#'     See `~torch.nn.Dropout` for details.
#'
#' @param p NA probability of an element to be zeroed. Default: 0.5
#' @param training NA apply dropout if is ``True``. Default: ``True``
#' @param inplace NA If set to ``True``, will do this operation in-place. Default: ``False``
#'
#' @name nnf_dropout
#'
#' @export
NULL


#' Dropout2d
#'
#' @section Randomly zero out entire channels (a channel is a 2D feature map, :
#'
#' Randomly zero out entire channels (a channel is a 2D feature map,
#'     e.g., the \eqn{j}-th channel of the \eqn{i}-th sample in the
#'     batched input is a 2D tensor \eqn{\text{input}[i, j]}) of the input tensor).
#'     Each channel will be zeroed out independently on every forward call with
#'     probability `p` using samples from a Bernoulli distribution.
#' 
#'     See `~torch.nn.Dropout2d` for details.
#'
#'
#' @param p NA probability of a channel to be zeroed. Default: 0.5
#' @param training NA apply dropout if is ``True``. Default: ``True``
#' @param inplace NA If set to ``True``, will do this operation in-place. Default: ``False``
#'
#' @name nnf_dropout2d
#'
#' @export
NULL


#' Dropout3d
#'
#' @section Randomly zero out entire channels (a channel is a 3D feature map, :
#'
#' Randomly zero out entire channels (a channel is a 3D feature map,
#'     e.g., the \eqn{j}-th channel of the \eqn{i}-th sample in the
#'     batched input is a 3D tensor \eqn{\text{input}[i, j]}) of the input tensor).
#'     Each channel will be zeroed out independently on every forward call with
#'     probability `p` using samples from a Bernoulli distribution.
#' 
#'     See `~torch.nn.Dropout3d` for details.
#'
#'
#' @param p NA probability of a channel to be zeroed. Default: 0.5
#' @param training NA apply dropout if is ``True``. Default: ``True``
#' @param inplace NA If set to ``True``, will do this operation in-place. Default: ``False``
#'
#' @name nnf_dropout3d
#'
#' @export
NULL

#' Embedding
#'
#' A simple lookup table that looks up embeddings in a fixed dictionary and size.
#' 
#'     This module is often used to retrieve word embeddings using indices.
#'     The input to the module is a list of indices, and the embedding matrix,
#'     and the output is the corresponding word embeddings.
#' 
#'     See `torch_nn.Embedding` for more details.
#'
#' @param input (LongTensor) Tensor containing indices into the embedding matrix
#' @param weight (Tensor) The embedding matrix with number of rows equal to the maximum possible index + 1,        and number of columns equal to the embedding size
#' @param padding_idx (int, optional) If given, pads the output with the embedding vector at `padding_idx`                                     (initialized to zeros) whenever it encounters the index.
#' @param max_norm (float, optional) If given, each embedding vector with norm larger than `max_norm`                                is renormalized to have norm `max_norm`.                                Note: this will modify `weight` in-place.
#' @param norm_type (float, optional) The p of the p-norm to compute for the `max_norm` option. Default ``2``.
#' @param scale_grad_by_freq (boolean, optional) If given, this will scale gradients by the inverse of frequency of                                            the words in the mini-batch. Default ``False``.
#' @param sparse (bool, optional) If ``True``, gradient w.r.t. `weight` will be a sparse tensor. See Notes under                             `torch_nn.Embedding` for more details regarding sparse gradients.
#'
#' @name nnf_embedding
#'
#' @export
NULL


#' Embedding_bag
#'
#' Computes sums, means or maxes of `bags` of embeddings, without instantiating the
#'     intermediate embeddings.
#' 
#'     See `torch_nn.EmbeddingBag` for more details.
#' 
#'     .. include:: cuda_deterministic_backward.rst
#'
#' @param input (LongTensor) Tensor containing bags of indices into the embedding matrix
#' @param weight (Tensor) The embedding matrix with number of rows equal to the maximum possible index + 1,        and number of columns equal to the embedding size
#' @param offsets (LongTensor, optional) Only used when `input` is 1D. `offsets` determines                         the starting index position of each bag (sequence) in `input`.
#' @param max_norm (float, optional) If given, each embedding vector with norm larger than `max_norm`                                is renormalized to have norm `max_norm`.                                Note: this will modify `weight` in-place.
#' @param norm_type (float, optional) The ``p`` in the ``p``-norm to compute for the `max_norm` option.                                 Default ``2``.
#' @param scale_grad_by_freq (boolean, optional) if given, this will scale gradients by the inverse of frequency of                                            the words in the mini-batch. Default ``False``.                                            Note: this option is not supported when ``mode="max"``.
#' @param mode (string, optional) ``"sum"``, ``"mean"`` or ``"max"``. Specifies the way to reduce the bag.                             Default: ``"mean"``
#' @param sparse (bool, optional) if ``True``, gradient w.r.t. `weight` will be a sparse tensor. See Notes under                             `torch_nn.Embedding` for more details regarding sparse gradients.                             Note: this option is not supported when ``mode="max"``.
#' @param per_sample_weights (Tensor, optional) a tensor of float / double weights, or None        to indicate all weights should be taken to be 1. If specified, `per_sample_weights`        must have exactly the same shape as input and is treated as having the same        `offsets`, if those are not None.
#' @param include_last_offset (bool, optional) if ``True``, the size of offsets is equal to the number of bags + 1.
#' @param The (sequence) 
#'
#' @name nnf_embedding_bag
#'
#' @export
NULL





#' Fractional_max_pool2d
#'
#' Applies 2D fractional max pooling over an input signal composed of several input planes.
#' 
#'     Fractional MaxPooling is described in detail in the paper `Fractional MaxPooling`_ by Ben Graham
#' 
#'     The max-pooling operation is applied in \eqn{kH \times kW} regions by a stochastic
#'     step size determined by the target output size.
#'     The number of output features is equal to the number of input planes.
#'
#' @param kernel_size NA the size of the window to take a max over.                 Can be a single number \eqn{k} (for a square kernel of \eqn{k \times k})                 or a tuple `(kH, kW)`
#' @param output_size NA the target output size of the image of the form \eqn{oH \times oW}.                 Can be a tuple `(oH, oW)` or a single number \eqn{oH} for a square image \eqn{oH \times oH}
#' @param output_ratio NA If one wants to have an output size as a ratio of the input size, this option can be given.                  This has to be a number or tuple in the range (0, 1)
#' @param return_indices NA if ``True``, will return the indices along with the outputs.                    Useful to pass to [`~torch.nn.functional.max_unpool2d`].
#'
#' @name nnf_fractional_max_pool2d
#'
#' @export
NULL


#' Fractional_max_pool2d_with_indices
#'
#' Applies 2D fractional max pooling over an input signal composed of several input planes.
#' 
#'     Fractional MaxPooling is described in detail in the paper `Fractional MaxPooling`_ by Ben Graham
#' 
#'     The max-pooling operation is applied in \eqn{kH \times kW} regions by a stochastic
#'     step size determined by the target output size.
#'     The number of output features is equal to the number of input planes.
#'
#' @param kernel_size NA the size of the window to take a max over.                 Can be a single number \eqn{k} (for a square kernel of \eqn{k \times k})                 or a tuple `(kH, kW)`
#' @param output_size NA the target output size of the image of the form \eqn{oH \times oW}.                 Can be a tuple `(oH, oW)` or a single number \eqn{oH} for a square image \eqn{oH \times oH}
#' @param output_ratio NA If one wants to have an output size as a ratio of the input size, this option can be given.                  This has to be a number or tuple in the range (0, 1)
#' @param return_indices NA if ``True``, will return the indices along with the outputs.                    Useful to pass to [`~torch.nn.functional.max_unpool2d`].
#'
#' @name nnf_fractional_max_pool2d_with_indices
#'
#' @export
NULL


#' Fractional_max_pool3d
#'
#' Applies 3D fractional max pooling over an input signal composed of several input planes.
#' 
#'     Fractional MaxPooling is described in detail in the paper `Fractional MaxPooling`_ by Ben Graham
#' 
#'     The max-pooling operation is applied in \eqn{kT \times kH \times kW} regions by a stochastic
#'     step size determined by the target output size.
#'     The number of output features is equal to the number of input planes.
#'
#' @param kernel_size NA the size of the window to take a max over.                 Can be a single number \eqn{k} (for a square kernel of \eqn{k \times k \times k})                 or a tuple `(kT, kH, kW)`
#' @param output_size NA the target output size of the form \eqn{oT \times oH \times oW}.                 Can be a tuple `(oT, oH, oW)` or a single number \eqn{oH} for a cubic output                  \eqn{oH \times oH \times oH}
#' @param output_ratio NA If one wants to have an output size as a ratio of the input size, this option can be given.                  This has to be a number or tuple in the range (0, 1)
#' @param return_indices NA if ``True``, will return the indices along with the outputs.                    Useful to pass to [`~torch.nn.functional.max_unpool3d`].
#'
#' @name nnf_fractional_max_pool3d
#'
#' @export
NULL


#' Fractional_max_pool3d_with_indices
#'
#' Applies 3D fractional max pooling over an input signal composed of several input planes.
#' 
#'     Fractional MaxPooling is described in detail in the paper `Fractional MaxPooling`_ by Ben Graham
#' 
#'     The max-pooling operation is applied in \eqn{kT \times kH \times kW} regions by a stochastic
#'     step size determined by the target output size.
#'     The number of output features is equal to the number of input planes.
#'
#' @param kernel_size NA the size of the window to take a max over.                 Can be a single number \eqn{k} (for a square kernel of \eqn{k \times k \times k})                 or a tuple `(kT, kH, kW)`
#' @param output_size NA the target output size of the form \eqn{oT \times oH \times oW}.                 Can be a tuple `(oT, oH, oW)` or a single number \eqn{oH} for a cubic output                  \eqn{oH \times oH \times oH}
#' @param output_ratio NA If one wants to have an output size as a ratio of the input size, this option can be given.                  This has to be a number or tuple in the range (0, 1)
#' @param return_indices NA if ``True``, will return the indices along with the outputs.                    Useful to pass to [`~torch.nn.functional.max_unpool3d`].
#'
#' @name nnf_fractional_max_pool3d_with_indices
#'
#' @export
NULL



#' Grad
#'
#' Gradient interface
#'
#'
#'
#' @name nnf_grad
#'
#' @export
NULL


#' Grid_sample
#'
#' Given an `input` and a flow-field `grid`, computes the
#'     ``output`` using `input` values and pixel locations from `grid`.
#' 
#'     Currently, only spatial (4-D) and volumetric (5-D) `input` are
#'     supported.
#' 
#'     In the spatial (4-D) case, for `input` with shape
#'     \eqn{(N, C, H_\text{in}, W_\text{in})} and `grid` with shape
#'     \eqn{(N, H_\text{out}, W_\text{out}, 2)}, the output will have shape
#'     \eqn{(N, C, H_\text{out}, W_\text{out})}.
#' 
#'     For each output location ``output[n, :, h, w]``, the size-2 vector
#'     ``grid[n, h, w]`` specifies `input` pixel locations ``x`` and ``y``,
#'     which are used to interpolate the output value ``output[n, :, h, w]``.
#'     In the case of 5D inputs, ``grid[n, d, h, w]`` specifies the
#'     ``x``, ``y``, ``z`` pixel locations for interpolating
#'     ``output[n, :, d, h, w]``. `mode` argument specifies ``nearest`` or
#'     ``bilinear`` interpolation method to sample the input pixels.
#' 
#'     `grid` specifies the sampling pixel locations normalized by the
#'     `input` spatial dimensions. Therefore, it should have most values in
#'     the range of ``[-1, 1]``. For example, values ``x = -1, y = -1`` is the
#'     left-top pixel of `input`, and values  ``x = 1, y = 1`` is the
#'     right-bottom pixel of `input`.
#' 
#'     If `grid` has values outside the range of ``[-1, 1]``, the corresponding
#'     outputs are handled as defined by `padding_mode`. Options are
#' 
#'         * ``padding_mode="zeros"``: use ``0`` for out-of-bound grid locations,
#'         * ``padding_mode="border"``: use border values for out-of-bound grid locations,
#'         * ``padding_mode="reflection"``: use values at locations reflected by
#'           the border for out-of-bound grid locations. For location far away
#'           from the border, it will keep being reflected until becoming in bound,
#'           e.g., (normalized) pixel location ``x = -3.5`` reflects by border ``-1``
#'           and becomes ``x' = 1.5``, then reflects by border ``1`` and becomes
#'           ``x'' = -0.5``.
#' 
#'     .. note::
#'         This function is often used in conjunction with [`affine_grid`]
#'         to build `Spatial Transformer Networks`_ .
#'     .. include:: cuda_deterministic_backward.rst
#'
#' @param input (Tensor) input of shape \eqn{(N, C, H_\text{in}, W_\text{in})} (4-D case)                    or \eqn{(N, C, D_\text{in}, H_\text{in}, W_\text{in})} (5-D case)
#' @param grid (Tensor) flow-field of shape \eqn{(N, H_\text{out}, W_\text{out}, 2)} (4-D case)                   or \eqn{(N, D_\text{out}, H_\text{out}, W_\text{out}, 3)} (5-D case)
#' @param mode (str) interpolation mode to calculate output values        ``'bilinear'`` | ``'nearest'``. Default: ``'bilinear'``
#' @param padding_mode (str) padding mode for outside grid values        ``'zeros'`` | ``'border'`` | ``'reflection'``. Default: ``'zeros'``
#' @param align_corners (bool, optional) Geometrically, we consider the pixels of the        input  as squares rather than points.        If set to ``True``, the extrema (``-1`` and ``1``) are considered as referring        to the center points of the input's corner pixels. If set to ``False``, they        are instead considered as referring to the corner points of the input's corner        pixels, making the sampling more resolution agnostic.        This option parallels the ``align_corners`` option in        [`interpolate`], and so whichever option is used here        should also be used there to resize the input image before grid sampling.        Default: ``False``
#'
#' @name nnf_grid_sample
#'
#' @export
NULL


#' Group_norm
#'
#' Applies Group Normalization for last certain number of dimensions.
#' 
#'     See `~torch.nn.GroupNorm` for details.
#'
#'
#'
#' @name nnf_group_norm
#'
#' @export
NULL


#' Handle_torch_function
#'
#' Implement a function with checks for __torch_function__ overrides.
#' 
#'     See torch::autograd::handle_torch_function for the equivalent of this
#'     function in the C++ implementation.
#' 
#'     Arguments
#'     ---------
#'     public_api : function
#'         Function exposed by the public torch API originally called like
#'         ``public_api(*args, **kwargs)`` on which arguments are now being
#'         checked.
#'     relevant_args : iterable
#'         Iterable of arguments to check for __torch_function__ methods.
#'     args : tuple
#'         Arbitrary positional arguments originally passed into ``public_api``.
#'     kwargs : tuple
#'         Arbitrary keyword arguments originally passed into ``public_api``.
#' 
#'     Returns
#'     -------
#'     Result from calling `implementation()` or an `__torch_function__`
#'     method, as appropriate.
#' 
#'     Raises
#'     ------
#'     TypeError : if no implementation is found.
#'
#'
#'
#' @name nnf_handle_torch_function
#'
#' @export
NULL





#' Hardsigmoid
#'
#' @section hardsigmoid(input) -> Tensor :
#'
#' Applies the element-wise function \eqn{\text{Hardsigmoid}(x) = \frac{ReLU6(x + 3)}{6}}
#'
#'
#' @param inplace NA If set to ``True``, will do this operation in-place. Default: ``False``
#'
#' @name nnf_hardsigmoid
#'
#' @export
NULL


#' Hardtanh
#'
#' @section hardtanh(input, min_val=-1., max_val=1., inplace=False) -> Tensor :
#'
#' Applies the HardTanh function element-wise. See `~torch.nn.Hardtanh` for more
#'     details.
#'
#'
#'
#'
#' @name nnf_hardtanh
#'
#' @export
NULL


#' Hardtanh_
#'
#' @section hardtanh_(input, min_val=-1., max_val=1.) -> Tensor :
#'
#' In-place version of [`~hardtanh`].
#'
#'
#'
#'
#' @name nnf_hardtanh_
#'
#' @export
NULL


#' Has_torch_function
#'
#' Check for __torch_function__ implementations in the elements of an iterable
#' 
#'     Arguments
#'     ---------
#'     relevant_args : iterable
#'         Iterable or aguments to check for __torch_function__ methods.
#' 
#'     Returns
#'     -------
#'     True if any of the elements of relevant_args have __torch_function__
#'     implementations, False otherwise.
#'
#'
#'
#' @name nnf_has_torch_function
#'
#' @export
NULL


#' Hinge_embedding_loss
#'
#' @section hinge_embedding_loss(input, target, margin=1.0, size_average=None, reduce=None, reduction='mean') -> Tensor :
#'
#' See `~torch.nn.HingeEmbeddingLoss` for details.
#'
#'
#'
#'
#' @name nnf_hinge_embedding_loss
#'
#' @export
NULL


#' Instance_norm
#'
#' Applies Instance Normalization for each channel in each data sample in a
#'     batch.
#' 
#'     See `~torch.nn.InstanceNorm1d`, `~torch.nn.InstanceNorm2d`,
#'     `~torch.nn.InstanceNorm3d` for details.
#'
#'
#'
#' @name nnf_instance_norm
#'
#' @export
NULL


#' Interpolate
#'
#' Down/up samples the input to either the given `size` or the given
#'     `scale_factor`
#' 
#'     The algorithm used for interpolation is determined by `mode`.
#' 
#'     Currently temporal, spatial and volumetric sampling are supported, i.e.
#'     expected inputs are 3-D, 4-D or 5-D in shape.
#' 
#'     The input dimensions are interpreted in the form:
#'     `mini-batch x channels x [optional depth] x [optional height] x width`.
#' 
#'     The modes available for resizing are: `nearest`, `linear` (3D-only),
#'     `bilinear`, `bicubic` (4D-only), `trilinear` (5D-only), `area`
#'
#' @param input (Tensor) the input tensor
#' @param size (int or Tuple[int] or Tuple[int, int] or Tuple[int, int, int]) output spatial size.
#' @param scale_factor (float or Tuple[float]) multiplier for spatial size. Has to match input size if it is a tuple.
#' @param mode (str) algorithm used for upsampling:        ``'nearest'`` | ``'linear'`` | ``'bilinear'`` | ``'bicubic'`` |        ``'trilinear'`` | ``'area'``. Default: ``'nearest'``
#' @param align_corners (bool, optional) Geometrically, we consider the pixels of the        input and output as squares rather than points.        If set to ``True``, the input and output tensors are aligned by the        center points of their corner pixels, preserving the values at the corner pixels.        If set to ``False``, the input and output tensors are aligned by the corner        points of their corner pixels, and the interpolation uses edge value padding        for out-of-boundary values, making this operation *independent* of input size        when `scale_factor` is kept the same. This only has an effect when `mode`        is ``'linear'``, ``'bilinear'``, ``'bicubic'`` or ``'trilinear'``.        Default: ``False``
#' @param recompute_scale_factor (bool, optional) recompute the scale_factor for use in the        interpolation calculation.  When `scale_factor` is passed as a parameter, it is used        to compute the `output_size`.  If `recompute_scale_factor` is ```True`` or not specified,        a new `scale_factor` will be computed based on the output and input sizes for use in the        interpolation computation (i.e. the computation will be identical to if the computed        `output_size` were passed-in explicitly).  Otherwise, the passed-in `scale_factor` will        be used in the interpolation computation.  Note that when `scale_factor` is floating-point,        the recomputed scale_factor may differ from the one passed in due to rounding and precision        issues.
#'
#' @name nnf_interpolate
#'
#' @export
NULL


#' Kl_div
#'
#' The `Kullback-Leibler divergence`_ Loss.
#' 
#'     See `~torch.nn.KLDivLoss` for details.
#'
#' @param input NA Tensor of arbitrary shape
#' @param target NA Tensor of the same shape as input
#' @param size_average (bool, optional) Deprecated (see `reduction`). By default,        the losses are averaged over each loss element in the batch. Note that for        some losses, there multiple elements per sample. If the field `size_average`        is set to ``False``, the losses are instead summed for each minibatch. Ignored        when reduce is ``False``. Default: ``True``
#' @param reduce (bool, optional) Deprecated (see `reduction`). By default, the        losses are averaged or summed over observations for each minibatch depending        on `size_average`. When `reduce` is ``False``, returns a loss per        batch element instead and ignores `size_average`. Default: ``True``
#' @param reduction (string, optional) Specifies the reduction to apply to the output:        ``'none'`` | ``'batchmean'`` | ``'sum'`` | ``'mean'``.        ``'none'``: no reduction will be applied        ``'batchmean'``: the sum of the output will be divided by the batchsize        ``'sum'``: the output will be summed        ``'mean'``: the output will be divided by the number of elements in the output        Default: ``'mean'``
#'
#' @name nnf_kl_div
#'
#' @export
NULL


#' L1_loss
#'
#' @section l1_loss(input, target, size_average=None, reduce=None, reduction='mean') -> Tensor :
#'
#' Function that takes the mean element-wise absolute value difference.
#' 
#'     See `~torch.nn.L1Loss` for details.
#'
#'
#'
#'
#' @name nnf_l1_loss
#'
#' @export
NULL


#' Layer_norm
#'
#' Applies Layer Normalization for last certain number of dimensions.
#' 
#'     See `~torch.nn.LayerNorm` for details.
#'
#'
#'
#' @name nnf_layer_norm
#'
#' @export
NULL

#' Linear
#'
#' Applies a linear transformation to the incoming data: \eqn{y = xA^T + b}.
#' 
#'     Shape:
#' 
#'         - Input: \eqn{(N, *, in\_features)} where `*` means any number of
#'           additional dimensions
#'         - Weight: \eqn{(out\_features, in\_features)}
#'         - Bias: \eqn{(out\_features)}
#'         - Output: \eqn{(N, *, out\_features)}
#'
#'
#'
#' @name nnf_linear
#'
#' @export
NULL


#' Local_response_norm
#'
#' Applies local response normalization over an input signal composed of
#'     several input planes, where channels occupy the second dimension.
#'     Applies normalization across channels.
#' 
#'     See `~torch.nn.LocalResponseNorm` for details.
#'
#'
#'
#' @name nnf_local_response_norm
#'
#' @export
NULL




#' Lp_pool1d
#'
#' Applies a 1D power-average pooling over an input signal composed of
#'     several input planes. If the sum of all inputs to the power of `p` is
#'     zero, the gradient is set to zero as well.
#' 
#'     See `~torch.nn.LPPool1d` for details.
#'
#'
#'
#' @name nnf_lp_pool1d
#'
#' @export
NULL


#' Lp_pool2d
#'
#' Applies a 2D power-average pooling over an input signal composed of
#'     several input planes. If the sum of all inputs to the power of `p` is
#'     zero, the gradient is set to zero as well.
#' 
#'     See `~torch.nn.LPPool2d` for details.
#'
#'
#'
#' @name nnf_lp_pool2d
#'
#' @export
NULL


#' Margin_ranking_loss
#'
#' @section margin_ranking_loss(input1, input2, target, margin=0, size_average=None, reduce=None, reduction='mean') -> Tensor :
#'
#' See `~torch.nn.MarginRankingLoss` for details.
#'
#'
#'
#'
#' @name nnf_margin_ranking_loss
#'
#' @export
NULL


#' Math
#'
#' This module provides access to the mathematical functions
#' defined by the C standard.
#'
#'
#'
#' @name nnf_math
#'
#' @export
NULL


#' Max_pool1d
#'
#' Applies a 1D max pooling over an input signal composed of several input
#'     planes.
#' 
#'     See `~torch.nn.MaxPool1d` for details.
#'
#'
#'
#' @name nnf_max_pool1d
#'
#' @export
NULL


#' Max_pool1d_with_indices
#'
#' Applies a 1D max pooling over an input signal composed of several input
#'     planes.
#' 
#'     See `~torch.nn.MaxPool1d` for details.
#'
#'
#'
#' @name nnf_max_pool1d_with_indices
#'
#' @export
NULL


#' Max_pool2d
#'
#' Applies a 2D max pooling over an input signal composed of several input
#'     planes.
#' 
#'     See `~torch.nn.MaxPool2d` for details.
#'
#'
#'
#' @name nnf_max_pool2d
#'
#' @export
NULL


#' Max_pool2d_with_indices
#'
#' Applies a 2D max pooling over an input signal composed of several input
#'     planes.
#' 
#'     See `~torch.nn.MaxPool2d` for details.
#'
#'
#'
#' @name nnf_max_pool2d_with_indices
#'
#' @export
NULL


#' Max_pool3d
#'
#' Applies a 3D max pooling over an input signal composed of several input
#'     planes.
#' 
#'     See `~torch.nn.MaxPool3d` for details.
#'
#'
#'
#' @name nnf_max_pool3d
#'
#' @export
NULL


#' Max_pool3d_with_indices
#'
#' Applies a 3D max pooling over an input signal composed of several input
#'     planes.
#' 
#'     See `~torch.nn.MaxPool3d` for details.
#'
#'
#'
#' @name nnf_max_pool3d_with_indices
#'
#' @export
NULL


#' Max_unpool1d
#'
#' Computes a partial inverse of `MaxPool1d`.
#' 
#'     See `~torch.nn.MaxUnpool1d` for details.
#'
#'
#'
#' @name nnf_max_unpool1d
#'
#' @export
NULL


#' Max_unpool2d
#'
#' Computes a partial inverse of `MaxPool2d`.
#' 
#'     See `~torch.nn.MaxUnpool2d` for details.
#'
#'
#'
#' @name nnf_max_unpool2d
#'
#' @export
NULL


#' Max_unpool3d
#'
#' Computes a partial inverse of `MaxPool3d`.
#' 
#'     See `~torch.nn.MaxUnpool3d` for details.
#'
#'
#'
#' @name nnf_max_unpool3d
#'
#' @export
NULL


#' Mse_loss
#'
#' @section mse_loss(input, target, size_average=None, reduce=None, reduction='mean') -> Tensor :
#'
#' Measures the element-wise mean squared error.
#' 
#'     See `~torch.nn.MSELoss` for details.
#'
#'
#'
#'
#' @name nnf_mse_loss
#'
#' @export
NULL





#' Multi_margin_loss
#'
#' @section multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None, :
#'
#' reduce=None, reduction='mean') -> Tensor
#' 
#'     See `~torch.nn.MultiMarginLoss` for details.
#'
#'
#'
#'
#' @name nnf_multi_margin_loss
#'
#' @export
NULL


#' Multilabel_margin_loss
#'
#' @section multilabel_margin_loss(input, target, size_average=None, reduce=None, reduction='mean') -> Tensor :
#'
#' See `~torch.nn.MultiLabelMarginLoss` for details.
#'
#'
#'
#'
#' @name nnf_multilabel_margin_loss
#'
#' @export
NULL


#' Multilabel_soft_margin_loss
#'
#' @section multilabel_soft_margin_loss(input, target, weight=None, size_average=None) -> Tensor :
#'
#' See `~torch.nn.MultiLabelSoftMarginLoss` for details.
#'
#'
#'
#'
#' @name nnf_multilabel_soft_margin_loss
#'
#' @export
NULL


#' Nll_loss
#'
#' The negative log likelihood loss.
#' 
#'     See `~torch.nn.NLLLoss` for details.
#'
#' @param input NA \eqn{(N, C)} where `C = number of classes` or \eqn{(N, C, H, W)}        in case of 2D Loss, or \eqn{(N, C, d_1, d_2, ..., d_K)} where \eqn{K \geq 1}        in the case of K-dimensional loss.
#' @param target NA \eqn{(N)} where each value is \eqn{0 \leq \text{targets}[i] \leq C-1},        or \eqn{(N, d_1, d_2, ..., d_K)} where \eqn{K \geq 1} for        K-dimensional loss.
#' @param weight (Tensor, optional) a manual rescaling weight given to each        class. If given, has to be a Tensor of size `C`
#' @param size_average (bool, optional) Deprecated (see `reduction`). By default,        the losses are averaged over each loss element in the batch. Note that for        some losses, there multiple elements per sample. If the field `size_average`        is set to ``False``, the losses are instead summed for each minibatch. Ignored        when reduce is ``False``. Default: ``True``
#' @param ignore_index (int, optional) Specifies a target value that is ignored        and does not contribute to the input gradient. When `size_average` is        ``True``, the loss is averaged over non-ignored targets. Default: -100
#' @param reduce (bool, optional) Deprecated (see `reduction`). By default, the        losses are averaged or summed over observations for each minibatch depending        on `size_average`. When `reduce` is ``False``, returns a loss per        batch element instead and ignores `size_average`. Default: ``True``
#' @param reduction (string, optional) Specifies the reduction to apply to the output:        ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,        ``'mean'``: the sum of the output will be divided by the number of        elements in the output, ``'sum'``: the output will be summed. Note: `size_average`        and `reduce` are in the process of being deprecated, and in the meantime,        specifying either of those two args will override `reduction`. Default: ``'mean'``
#'
#' @name nnf_nll_loss
#'
#' @export
NULL


#' Normalize
#'
#' Performs \eqn{L_p} normalization of inputs over specified dimension.
#' 
#'     For a tensor `input` of sizes \eqn{(n_0, ..., n_{dim}, ..., n_k)}, each
#'     \eqn{n_{dim}} -element vector \eqn{v} along dimension `dim` is transformed as
#' 
#' \deqn{
#'         v = \frac{v}{\max(\lVert v \rVert_p, \epsilon)}.
#' }
#'     With the default arguments it uses the Euclidean norm over vectors along dimension \eqn{1} for normalization.
#'
#' @param input NA input tensor of any shape
#' @param p (float) the exponent value in the norm formulation. Default: 2
#' @param dim (int) the dimension to reduce. Default: 1
#' @param eps (float) small value to avoid division by zero. Default: 1e-12
#' @param out (Tensor, optional) the output tensor. If `out` is used, this                            operation won't be differentiable.
#'
#' @name nnf_normalize
#'
#' @export
NULL


#' One_hot
#'
#' @section one_hot(tensor, num_classes=-1) -> LongTensor :
#'
#' Takes LongTensor with index values of shape ``(*)`` and returns a tensor
#' of shape ``(*, num_classes)`` that have zeros everywhere except where the
#' index of last dimension matches the corresponding value of the input tensor,
#' in which case it will be 1.
#' 
#' See also `One-hot on Wikipedia`_ .
#' 
#' .. _One-hot on Wikipedia:
#'     https://en.wikipedia.org/wiki/One-hot
#'
#'
#' @param tensor (LongTensor) class values of any shape.
#' @param num_classes (int) Total number of classes. If set to -1, the number        of classes will be inferred as one greater than the largest class        value in the input tensor.
#'
#' @name nnf_one_hot
#'
#' @export
NULL


#' Pad
#'
#' Pads tensor.
#' 
#'     Padding size:
#'         The padding size by which to pad some dimensions of `input`
#'         are described starting from the last dimension and moving forward.
#'         \eqn{\left\lfloor\frac{\text{len(pad)}}{2}\right\rfloor} dimensions
#'         of ``input`` will be padded.
#'         For example, to pad only the last dimension of the input tensor, then
#'         `pad` has the form
#'         \eqn{(\text{padding\_left}, \text{padding\_right})};
#'         to pad the last 2 dimensions of the input tensor, then use
#'         \eqn{(\text{padding\_left}, \text{padding\_right},}
#'         \eqn{\text{padding\_top}, \text{padding\_bottom})};
#'         to pad the last 3 dimensions, use
#'         \eqn{(\text{padding\_left}, \text{padding\_right},}
#'         \eqn{\text{padding\_top}, \text{padding\_bottom}}
#'         \eqn{\text{padding\_front}, \text{padding\_back})}.
#' 
#'     Padding mode:
#'         See `torch_nn.ConstantPad2d`, `torch.nn.ReflectionPad2d`, and
#'         `torch_nn.ReplicationPad2d` for concrete examples on how each of the
#'         padding modes works. Constant padding is implemented for arbitrary dimensions.
#'         Replicate padding is implemented for padding the last 3 dimensions of 5D input
#'         tensor, or the last 2 dimensions of 4D input tensor, or the last dimension of
#'         3D input tensor. Reflect padding is only implemented for padding the last 2
#'         dimensions of 4D input tensor, or the last dimension of 3D input tensor.
#' 
#'     .. include:: cuda_deterministic_backward.rst
#'
#' @param input (Tensor) N-dimensional tensor
#' @param pad (tuple) m-elements tuple, where        \eqn{\frac{m}{2} \leq} input dimensions and \eqn{m} is even.
#' @param mode NA ``'constant'``, ``'reflect'``, ``'replicate'`` or ``'circular'``.        Default: ``'constant'``
#' @param value NA fill value for ``'constant'`` padding. Default: ``0``
#'
#' @name nnf_pad
#'
#' @export
NULL


#' Pairwise_distance
#'
#' See `torch_nn.PairwiseDistance` for details
#'
#'
#'
#' @name nnf_pairwise_distance
#'
#' @export
NULL


#' Pdist
#'
#' @section pdist(input, p=2) -> Tensor :
#'
#' Computes the p-norm distance between every pair of row vectors in the input.
#' This is identical to the upper triangular portion, excluding the diagonal, of
#' `torch_norm(input[:, None] - input, dim=2, p=p)`. This function will be faster
#' if the rows are contiguous.
#' 
#' If input has shape \eqn{N \times M} then the output will have shape
#' \eqn{\frac{1}{2} N (N - 1)}.
#' 
#' This function is equivalent to `scipy.spatial.distance.pdist(input,
#' 'minkowski', p=p)` if \eqn{p \in (0, \infty)}. When \eqn{p = 0} it is
#' equivalent to `scipy.spatial.distance.pdist(input, 'hamming') * M`.
#' When \eqn{p = \infty}, the closest scipy function is
#' `scipy.spatial.distance.pdist(xn, lambda x, y: np.abs(x - y).max())`.
#'
#'
#' @param input NA input tensor of shape \eqn{N \times M}.
#' @param p NA p value for the p-norm distance to calculate between each vector pair        \eqn{\in [0, \infty]}.
#'
#' @name nnf_pdist
#'
#' @export
NULL


#' Pixel_shuffle
#'
#' @section Rearranges elements in a tensor of shape :math:`(*, C \times r^2, H, W)` to a :
#'
#' Rearranges elements in a tensor of shape \eqn{(*, C \times r^2, H, W)} to a
#' tensor of shape \eqn{(*, C, H \times r, W \times r)}.
#' 
#' See `~torch.nn.PixelShuffle` for details.
#'
#'
#' @param input (Tensor) the input tensor
#' @param upscale_factor (int) factor to increase spatial resolution by
#'
#' @name nnf_pixel_shuffle
#'
#' @export
NULL


#' Poisson_nll_loss
#'
#' Poisson negative log likelihood loss.
#' 
#'     See `~torch.nn.PoissonNLLLoss` for details.
#'
#' @param input NA expectation of underlying Poisson distribution.
#' @param target NA random sample \eqn{target \sim \text{Poisson}(input)}.
#' @param log_input NA if ``True`` the loss is computed as        \eqn{\exp(\text{input}) - \text{target} * \text{input}}, if ``False`` then loss is        \eqn{\text{input} - \text{target} * \log(\text{input}+\text{eps})}. Default: ``True``
#' @param full NA whether to compute full loss, i. e. to add the Stirling        approximation term. Default: ``False``        \eqn{\text{target} * \log(\text{target}) - \text{target} + 0.5 * \log(2 * \pi * \text{target})}.
#' @param size_average (bool, optional) Deprecated (see `reduction`). By default,        the losses are averaged over each loss element in the batch. Note that for        some losses, there multiple elements per sample. If the field `size_average`        is set to ``False``, the losses are instead summed for each minibatch. Ignored        when reduce is ``False``. Default: ``True``
#' @param eps (float, optional) Small value to avoid evaluation of \eqn{\log(0)} when        `log_input`=``False``. Default: 1e-8
#' @param reduce (bool, optional) Deprecated (see `reduction`). By default, the        losses are averaged or summed over observations for each minibatch depending        on `size_average`. When `reduce` is ``False``, returns a loss per        batch element instead and ignores `size_average`. Default: ``True``
#' @param reduction (string, optional) Specifies the reduction to apply to the output:        ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,        ``'mean'``: the sum of the output will be divided by the number of        elements in the output, ``'sum'``: the output will be summed. Note: `size_average`        and `reduce` are in the process of being deprecated, and in the meantime,        specifying either of those two args will override `reduction`. Default: ``'mean'``
#'
#' @name nnf_poisson_nll_loss
#'
#' @export
NULL

#' Sigmoid
#'
#' @section sigmoid(input) -> Tensor :
#'
#' Applies the element-wise function \eqn{\text{Sigmoid}(x) = \frac{1}{1 + \exp(-x)}}
#' 
#'     See `~torch.nn.Sigmoid` for more details.
#'
#'
#'
#'
#' @name nnf_sigmoid
#'
#' @export
NULL


#' Smooth_l1_loss
#'
#' Function that uses a squared term if the absolute
#'     element-wise error falls below 1 and an L1 term otherwise.
#' 
#'     See `~torch.nn.SmoothL1Loss` for details.
#'
#'
#'
#' @name nnf_smooth_l1_loss
#'
#' @export
NULL


#' Soft_margin_loss
#'
#' @section soft_margin_loss(input, target, size_average=None, reduce=None, reduction='mean') -> Tensor :
#'
#' See `~torch.nn.SoftMarginLoss` for details.
#'
#'
#'
#'
#' @name nnf_soft_margin_loss
#'
#' @export
NULL



#' Tanh
#'
#' @section tanh(input) -> Tensor :
#'
#' Applies element-wise,
#'     \eqn{\text{Tanh}(x) = \tanh(x) = \frac{\exp(x) - \exp(-x)}{\exp(x) + \exp(-x)}}
#' 
#'     See `~torch.nn.Tanh` for more details.
#'
#'
#'
#'
#' @name nnf_tanh
#'
#' @export
NULL


#' Torch
#'
#' The torch package contains data structures for multi-dimensional
#' tensors and mathematical operations over these are defined.
#' Additionally, it provides many utilities for efficient serializing of
#' Tensors and arbitrary types, and other useful utilities.
#' 
#' It has a CUDA counterpart, that enables you to run your tensor computations
#' on an NVIDIA GPU with compute capability >= 3.0.
#'
#'
#'
#' @name nnf_torch
#'
#' @export
NULL


#' Triplet_margin_loss
#'
#' See `~torch.nn.TripletMarginLoss` for details
#'
#'
#'
#' @name nnf_triplet_margin_loss
#'
#' @export
NULL





#' Upsample
#'
#' Upsamples the input to either the given `size` or the given
#'     `scale_factor`
#' 
#'     .. warning::
#'         This function is deprecated in favor of [`torch_nn.functional.interpolate`].
#'         This is equivalent with ``nn.functional.interpolate(...)``.
#' 
#'     .. include:: cuda_deterministic_backward.rst
#' 
#'     The algorithm used for upsampling is determined by `mode`.
#' 
#'     Currently temporal, spatial and volumetric upsampling are supported, i.e.
#'     expected inputs are 3-D, 4-D or 5-D in shape.
#' 
#'     The input dimensions are interpreted in the form:
#'     `mini-batch x channels x [optional depth] x [optional height] x width`.
#' 
#'     The modes available for upsampling are: `nearest`, `linear` (3D-only),
#'     `bilinear`, `bicubic` (4D-only), `trilinear` (5D-only)
#'
#' @param input (Tensor) the input tensor
#' @param size (int or Tuple[int] or Tuple[int, int] or Tuple[int, int, int]) output spatial size.
#' @param scale_factor (float or Tuple[float]) multiplier for spatial size. Has to be an integer.
#' @param mode (string) algorithm used for upsampling:        ``'nearest'`` | ``'linear'`` | ``'bilinear'`` | ``'bicubic'`` |        ``'trilinear'``. Default: ``'nearest'``
#' @param align_corners (bool, optional) Geometrically, we consider the pixels of the        input and output as squares rather than points.        If set to ``True``, the input and output tensors are aligned by the        center points of their corner pixels, preserving the values at the corner pixels.        If set to ``False``, the input and output tensors are aligned by the corner        points of their corner pixels, and the interpolation uses edge value padding        for out-of-boundary values, making this operation *independent* of input size        when `scale_factor` is kept the same. This only has an effect when `mode`        is ``'linear'``, ``'bilinear'``, ``'bicubic'`` or ``'trilinear'``.        Default: ``False``
#'
#' @name nnf_upsample
#'
#' @export
NULL


#' Upsample_bilinear
#'
#' Upsamples the input, using bilinear upsampling.
#' 
#'     .. warning::
#'         This function is deprecated in favor of [`torch_nn.functional.interpolate`].
#'         This is equivalent with
#'         ``nn.functional.interpolate(..., mode='bilinear', align_corners=True)``.
#' 
#'     Expected inputs are spatial (4 dimensional). Use `upsample_trilinear` fo
#'     volumetric (5 dimensional) inputs.
#'
#' @param input (Tensor) input
#' @param size (int or Tuple[int, int]) output spatial size.
#' @param scale_factor (int or Tuple[int, int]) multiplier for spatial size
#'
#' @name nnf_upsample_bilinear
#'
#' @export
NULL


#' Upsample_nearest
#'
#' Upsamples the input, using nearest neighbours' pixel values.
#' 
#'     .. warning::
#'         This function is deprecated in favor of [`torch_nn.functional.interpolate`].
#'         This is equivalent with ``nn.functional.interpolate(..., mode='nearest')``.
#' 
#'     Currently spatial and volumetric upsampling are supported (i.e. expected
#'     inputs are 4 or 5 dimensional).
#'
#' @param input (Tensor) input
#' @param size (int or Tuple[int, int] or Tuple[int, int, int]) output spatia        size.
#' @param scale_factor (int) multiplier for spatial size. Has to be an integer.
#'
#' @name nnf_upsample_nearest
#'
#' @export
NULL


#' Warnings
#'
#' Python part of the warnings subsystem.
#'
#'
#'
#' @name nnf_warnings
#'
#' @export
NULL
